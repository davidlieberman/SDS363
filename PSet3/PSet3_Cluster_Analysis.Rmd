---
title: "PSet 3"
author: "Ramiz Çolak, Ryo Tamaki, David Lieberman, Liana Wang"
date: "2/21/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(aplpack)
library(fpc)
library(cluster)
library(ape)
library(tidyverse)
```

## Introduction to Our Data set

**We're using a dataset on the New York Stock Exchange, which we obtained from Kaggle. There are 79 columns of variables measuring various aspects of stock performance of the S&P 500, with data from the past few years. In every row, we have a company, and each row name is set to the company's stock identifying ticker. Column names include various characteristics about the company ranging from Earnings and Total Costs, to Profit Marging and Price/Earnings ratio. For our cluster analysis, we formed a subset of this data set. For this, we selected 6 columns easily comparable across companies: Cash Ratio, Gross Margin, Operating Margin, Profit Margin, Quick Ratio, and Earnings Per Share.**

**These 6 ratio measures tell more about the profitability / cost management of the company through Profit Margin, Operating Margin, Gross Margin and Earnings Per Share. Furthermore, they tell about the leverage risk of the company through Quick Ratio and Cash Ratio--liquidity ratios that tell more about how easily a company can service its debt.**

**When we look at the selected 45 rows, each of which represents a publicly-traded company, we see that they come from a range of industries. They represent approximately 7 different industries. It is known that each industry has its own unique average profit margins and liquidity ratios. For example, AMGN (Amgen) is a biotech firm that has much higher debt levels, which is normal for the biotech industry. While AMZN (Amazon) is a tech/consumers company that has higher profit margins and relatively less debt burden, which is also normal for a consumers company. Given that we have 7 industries in this subsetted data set, and each industry has a unique character, our hypothesis is that we should have between 6-8 clusters. We will test whether this is true through hierarchical and k-means cluster analysis.**

```{r}
# Reading the Data
new_data <- read.csv("https://pastebin.com/raw/bNrAceXf")
```

```{r}
# Selecting Relevant Columns
colnames(new_data)
index3 <- c(5, 9, 19, 20, 21, 26)

# Selecting 45 Rows
rowindex <- seq(1, 89, by = 2)

data_1 <- new_data[rowindex,]

# Changing the Row Names to Tickers of the Company
data_1 <- data_1 %>% remove_rownames %>% column_to_rownames(var="Ticker.Symbol")

# Subsetting by 6 Columns
data_2 <- data_1[,index3]
```


## PROBLEM 1
1.	Think about what metrics are appropriate for your data based on data type. Write a few sentences about this.  Also think about whether you should standardize or transform your data (comment as appropriate).

```{r}
# Scaling the data
data_f <- scale(data_2)
```

**Before we started our analysis, we standardized our data. For this we used the scale function in R. A standardized dataset is necessary mainly because we are calculating distance in the cluster analysis. Non-scaled columns could distort the distance calculations and offer a final clustering that does not have much explanatory power over our data.**

**The columns of our subsetted data set includes only continuous variables. For this data set, for distance metrics, Euclidean or Manhattan would work well. Below in our analysis, we saw that Euclidean distance offers more descriptive clusters that, therefore for most of this analysis, we preferred Euclidean distance.**

**Furthermore, because each company is unique and our data set includes all sorts of companies, there are likely to be outliers. Because simple linkage is more sensitive to outliers, agglemeration methods that are less sensitive to outliers would work better for our data set. Complete linkage is an example method that is suitable.**


## PROBLEM 2
2.	Try various forms of hierarchical cluster analysis.  Try at least two different metrics and two agglomeration methods.  Produce dendrograms and comment on what you observe.

#### Model 1
Euclidean Distance + Complete
```{r}
# Get distance matrix
data_dist <- dist(data_f[,-1], method="euclidean")

# Perform cluster analysis
data_clust <- hclust(data_dist, method = "complete")

# Make dendrogram
plot(data_clust, xlab="",ylab="Distance", cex = .5, main="Clustering for Stocks")

# Identify groups
#rect.hclust(data_clust, k=5)
rect.hclust(data_clust,k=7)
```
**In this first model distance was calculated using the Euclidean method and a complete agglomeration method. The last command asks R to identify 7 clusters, since our data contains roughly 7 industries, and we think that might influence variables like risk and profit margin. From the dendogram, though, we can see three relatively larger clusters and four small single-stock clusters (CF, CMG, BIIB, APA). The left-most large cluster on the tree contains a variety of stocks, including those from companies in communications and manufacturing. The second cluster is smaller, but more specific to medical and other technology. The last cluster has a large number of stocks related to utilities and household products.**

Plain dendogram
```{r}
plot(data_clust,  main = "Plain dendrogram", hang = -1, cex = .5,
     xlab = "", ylab = "", sub = "", axes = FALSE)
```

#### Model 2

```{r}
# Get distance matrix
data_dist_2 <- dist(data_f[,-1], method = "euclidean")

# Perform cluster analysis
data_clust_2 <- hclust(data_dist_2, method = "single")

# Make dendrogram
plot(data_clust_2, xlab="",ylab="Distance",main="Clustering for Stocks")

# Identify groups
rect.hclust(data_clust_2, k=7)
```

**Model 2 used a "single" agglomeration method, which ought to produce more distinct groups. In this case, however, we obtained one large cluster that includes most of the stocks, and a few scattered ones toward the left-hand side of the dendogram. After seeing the results of single agglomeration, it does not appear to be particularly useful for cluster analysis, so we decided to proceed without using this agglomeration method again.**

#### Model 3

Manhattan + Complete
```{r}
# Get distance matrix
data_dist_3 <- dist(data_f[,-1], method = "manhattan")

# Perform cluster analysis
data_clust_3 <- hclust(data_dist_3, method = "complete")

# Make dendrogram
plot(data_clust_3, xlab="",ylab="Distance",main="Clustering for Stocks", cex = .5)

# Identify groups
rect.hclust(data_clust_3,k=7)
```
**Since Manhattan distance calculation could work well for high-dimensional datasets, and also places less weight upon outliers, we also decided to see whether a different distance calculation would produce different clusters from the Euclidean distance method.**

#### Model 5

Euclidean + Average

```{r}
# Get distance matrix
data_dist_5 <- dist(data_f[,-1], method = "euclidean")
# Perform cluster analysis
data_clust_5 <- hclust(data_dist_5, method = "average")
# Make dendrogram
plot(data_clust_5, xlab="",ylab="Distance",main="Clustering for Stocks")
# Identify three groups
rect.hclust(data_clust_5, k=5)
```

Plain dendogram for Model 5
```{r}
plot(data_clust_5,  main = "Plain dendrogram", hang = -1, cex = .5,
     xlab = "", ylab = "", sub = "", axes = FALSE)
```

#### Model 6

Manhattan + Average

```{r}
# Get distance matrix
data_dist_6 <- dist(data_f[,-1], method = "euclidean")
# Perform cluster analysis
data_clust_6 <- hclust(data_dist_6, method = "average")
# Make dendrogram
plot(data_clust_6, xlab="",ylab="Distance", main="Clustering for Stocks")
# Identify three groups
rect.hclust(data_clust_6, k=5)
```

Plain dendogram for Model 6
```{r}
plot(data_clust_6,  main = "Plain dendrogram", hang = -1, cex = .5,
     xlab = "", ylab = "", sub = "", axes = FALSE)
```


## PROBLEM 3
3.	If possible, run the SAS macro  or the R function to think about how many groups you want to retain (i.e. get the plots of cluster distance, R-squared, etc).  If you can’t run this, discuss how many groups you think are present.

```{r}
# We utilise the Euclidian distance method to compute distances and intiate clustering. Although we are cognisant of the succeptibility of Euclidean distance to outliers, we note that our data does not appear to have significant outliers, as revealed after plotting boxplots of our variables.

dist1 <- dist(data_f, method="euclidean")

# We run the hclust() command to create the Hierarchical Ward Cluster (using Euclidean distance). We utilise Ward's method as "Several studies of clustering suggest that Ward’s method or Average Linkage work the ‘best’" (pg. 324, Cluster Analysis Lecture)
clust1 <- hclust(dist1, method="ward.D")

# We draw the dendogram of our data. We opt for 5 clusters to begin with as we hypothesise that the stocks could potentially be divided along industry lines and upon cursory glance, there are about 5 categories. (NB: We later find that there are more than 5 categories, 7 to be precise, but this was made after conducting the data anlaysis from below and by looking at the companies in greater detail.) We therefore opt for 5 clusters and delineate them with the red lines as such.
plot(clust1, cex=.9, xlab="",ylab="Distance",main="Clustering for NYSE Data")
rect.hclust(clust1,k=7)
```

```{r}
# We evaluate the number of groups that we wish to retain by calculating the 'guidance statistics,' namely: R-Squared, Root-mean-square standard deviation, semi-partial R-squared, and cluster distance. In computing these statistics and producing the plot, we find that
source("http://reuningscherer.net/stat660/R/HClusEval.R.txt")

hclus_eval(data_f, dist_m = 'euclidean', clus_m = 'ward', plot_op = T)
```

## PROBLEM 4
4.	Run k-means clustering on your data.  Compare results to what you got in 3.)  Include a sum of squares vs. k (number of clusters) plot and comment on how many groups exist.

```{r}
# kdata is just normalized input dataset
kdata <- data_f
# Set max value for number of clusters k
n.lev <- 15
# Calculate the within groups sum of squared error (SSE) for the number of cluster solutions selected by the user
wss <- rnorm(10)
while (prod(wss==sort(wss,decreasing=T))==0) {
  wss <- (nrow(kdata)-1)*sum(apply(kdata,2,var))
  for (i in 2:n.lev) wss[i] <- sum(kmeans(kdata, centers=i)$withinss)}
# Calculate the within groups SSE for 250 randomized data sets (based on the original input data)
k.rand <- function(x){
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}
rand.mat <- matrix(0,n.lev,250)
k.1 <- function(x) {
  for (i in 1:250) {
    r.mat <- as.matrix(suppressWarnings(k.rand(kdata)))
    rand.mat[,i] <- r.mat}
  return(rand.mat)
}
# Same function as above for data with < 3 column variables
k.2.rand <- function(x){
  rand.mat <- matrix(0,n.lev,250)
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}
k.2 <- function(x){
  for (i in 1:250) {
    r.1 <- k.2.rand(kdata)
    rand.mat[,i] <- r.1}
  return(rand.mat)
}
# Determine if the data data table has > or < 3 variables and call appropriate function above
if (dim(kdata)[2] == 2) { rand.mat <- k.2(kdata) } else { rand.mat <- k.1(kdata) }
# Plot within groups SSE against all tested cluster solutions for actual and randomized data - 1st: Log scale, 2nd: Normal scale
xrange <- range(1:n.lev)
yrange <- range(log(rand.mat),log(wss))
plot(xrange,yrange, type='n', xlab='Cluster Solution', ylab='Log of Within Group SSE', main='Cluster Solutions against Log of SSE')
for (i in 1:250) lines(log(rand.mat[,i]),type='l',col='red')
lines(log(wss), type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)
```
```{r}
# Calculate the mean and standard deviation of difference between SSE of actual data and SSE of 250 randomized datasets
r.sse <- matrix(0,dim(rand.mat)[1],dim(rand.mat)[2])
wss.1 <- as.matrix(wss)
for (i in 1:dim(r.sse)[2]) {
  r.temp <- abs(rand.mat[,i]-wss.1[,1])
  r.sse[,i] <- r.temp}
r.sse.m <- apply(r.sse,1,mean)
r.sse.sd <- apply(r.sse,1,sd)
r.sse.plus <- r.sse.m + r.sse.sd
r.sse.min <- r.sse.m - r.sse.sd

# Plot differeince between actual SSE mean SSE from 250 randomized datasets - 1st: Log scale, 2nd: Normal scale

xrange <- range(1:n.lev)
yrange <- range(log(r.sse.plus),log(r.sse.min))
plot(xrange,yrange, type='n',xlab='Cluster Solution', ylab='Log of SSE - Random SSE', main='Cluster Solustions against (Log of SSE - Random SSE)')
lines(log(r.sse.m), type="b", col='blue')
lines(log(r.sse.plus), type='l', col='red')
lines(log(r.sse.min), type='l', col='red')
legend('topright',c('SSE - random SSE', 'SD of SSE-random SSE'), col=c('blue', 'red'), lty=1)
```

Deciding on number of levels to keep

```{r}
clust.level <- 7
# Apply K-means cluster solutions - append clusters to CSV file
fit <- kmeans(na.omit(kdata), clust.level)
aggregate(kdata, by=list(fit$cluster), FUN=mean)
```

```{r}
clust.out <- fit$cluster
kclust <- as.matrix(clust.out)
kclust.out <- cbind(kclust, data_f)
write.table(kclust.out, file="kmeans_out.csv", sep=",")
# Display Principal Components plot of data with clusters identified
clusplot(kdata, fit$cluster, shade=F, labels=2, lines=0, color=T, lty=4, main='Principal Components plot showing K-means clusters')
```

```{r}
plotcluster(kdata, fit$cluster, main="Five Cluster Solution in DA Space",
            xlab="First Discriminant Function", ylab="Second Discriminant Function")
```

## PROBLEM 5
5.	Comment on the number of groups that seem to be present based on what you find above.

List of industries:
1) manufacturing 2) pharma/health 3) tech 4) transportation 5) utilities
6) oil and gas 7) communications

1. AAL - American Airlines Group - airline
2. ABBV - AbbVie Inc - pharma
3. ABT - Abbott Laboratories- healthcare
4. ADM - Archer Daniels - food
5. ADS - Alliance Data Systems Corp  - marketing
6. AEE - Ameren Corp - utilities
7. AKAM - Akamai Tech - media/tech
8. ALB - Albemarle Corporation - chemicals
9. ALK - Alaska air - airline
10. ALLE - Allegion PLC - security tech
11. ALXN - Alexion Pharma - pharma
12. AME - African Methodist Episcopal - church
13. AMGN - Amgen Inc - biopharma
14. AMT - American Tower Corp - comms
15. AMZN - Amazon - tech
16. AN - AutoNation - cars
17. APA - Apache Corporation - oil & gas
18. APC - Anadarko Petroleum - oil & gas
19. APH - Amphenol Corporation - electronics
20. ARNC - Arconic Inc - manufacturing
21. ATVI - Activision Blizzard,
22. AWK - American Water Works Co - utilities
23. BA - Boeing Co - manufacturing
24. BAX - Baxter International Inc - medical products
25. BCR - CR Bard - biotech
26. BHI - Baker Hughes Inc - oil and gas
27. BIIB - Biogen Inc - biotech
28. BLL - Ball Corporation - manufacturing
29. BMY - Bristol-Myers Squibb - pharma
30. BSX - Boston Scientific Corporation - biotech
31. BWA - BorgWarner Inc - cars
32. CAT - Caterpillar Inc - manufacturing
33. CBG - CBRE Group - real estate
34. CCI - Crown Castle IN/SH - comms
35. CELG - Celgene Corporation - biopharma for cancer and inflammation
36. CF - CF Industries Holdings, Inc - chemicals/agricultural fertilizers
37. CHD - Church & Dwight Co, Inc - household products
38. CHK - Chesapeake Energy Corporation - oil and gas
39. CHRW - C.H. Robinson Worldwide Inc - multimodal transportation and storage
40. CHTR - Charter Communications Inc - media/comms
41. CL - Colgate-Palmolive Company - health care
42. CMG - Chipotle Mexican Grill, Inc - food
43. CMI - Cummins Inc - utilities
44. CMS - CMS Energy Corporation - utilities
45. CNP - CenterPoint Energy, Inc - utilities
