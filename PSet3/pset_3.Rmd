---
title: "PSet 3"
author: "Ramiz Çolak, Ryo Tamaki, David Lieberman, Liana Wang"
date: "2/21/2019"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(aplpack)
library(fpc)
library(cluster)
library(ape)
library(tidyverse)
```

## PROBLEM 1
1.	Think about what metrics are appropriate for your data based on data type. Write a few sentences about this.  Also think about whether you should standardize or transform your data (comment as appropriate).

```{r}
#Reading the Data
new_data <- read.csv("https://pastebin.com/raw/bNrAceXf")
```

```{r}
#Selecting Relevant Columns
colnames(new_data)
index3 <- c(5, 9, 19, 20, 21, 26)

#Selecting 45 Rows
rowindex <- seq(1, 89, by = 2)

data_1 <- new_data[rowindex,]

#Changing the Row Names to Tickers of the Company
data_1 <- data_1 %>% remove_rownames %>% column_to_rownames(var="Ticker.Symbol")

#Subsetting by 6 Columns
data_2 <- data_1[,index3]

#Scaling the data
data_f <- scale(data_2)
```

We selected following 6 columns: Cash Ratio, Gross Margin, Operating Margin, Profit Margin, Quick Ratio, and Earnings Per Share. The reason we selected these 6 columns is that they are "ratio" measures, that is, they are comparable metrics for each of the listed companies. All companies have margins and ratio that fall on a similar range, and this makes them groupable. Since we would love to be able to group closer data points, we want to use comparable units, and these columns fulfill this requirement.

After we selected our columns, we saw that our columns only include continuous variables. Therefore, overall, we preferred using Euclidean distance. That being said, to try different distance measures, in question 2, we also included Manhattan distance--which is also suitable for continuous variables.

In terms of agglomeration methods, our data set is more suitable for complete linkage or average linkage. Because companies may have very different underlying fundamentals, it is possible to have outliers. Hence, we prefer agglomeration methods that are less sensitive to outliers.

## PROBLEM 2
2.	Try various forms of hierarchical cluster analysis.  Try at least two different metrics and two agglomeration methods.  Produce dendrograms and comment on what you observe.

***How do we choose the optimal number of clusters for hierarchical***???

**Note that our data is mostly continuous. We are going to try two distance metrics and three agglomeration methods. Distance methods we used are Euclidean and Manhattan, and agglomeration methods we used are complete, single and average. This gives us six different cases.**

##Model 1
Euclidean Distance + Complete
```{r}
#Get distance matrix
data_dist <- dist(data_f[,-1], method="euclidean")

#Perform cluster analysis
data_clust <- hclust(data_dist, method = "complete")

#Make dendrogram
plot(data_clust, xlab="",ylab="Distance", cex = .5, main="Clustering for Stocks")

#identify five groups
rect.hclust(data_clust,k=7)
```
**In the dendogram above, we decided to ask R**

Plain dendogram
```{r}
plot(data_clust,  main = "Plain dendrogram", hang = -1, cex = .5,
     xlab = "", ylab = "", sub = "", axes = FALSE)
```

##Model 2

```{r}
#Get distance matrix
data_dist_2 <- dist(data_f[,-1], method = "euclidean")

#Perform cluster analysis
data_clust_2 <- hclust(data_dist_2, method = "single")

#Make dendrogram
plot(data_clust_2, xlab="",ylab="Distance",main="Clustering for Stocks")

#identify three groups
rect.hclust(data_clust_2, k=5)
```

Plain dendogram for Model 2
```{r}
plot(data_clust_2,  main = "Plain dendrogram", hang = -1, cex = .5,
     xlab = "", ylab = "", sub = "", axes = FALSE)
```

##Model 3

Manhattan + Complete
```{r}
#Get distance matrix
data_dist_3 <- dist(data_f[,-1], method = "manhattan")

#Perform cluster analysis
data_clust_3 <- hclust(data_dist_3, method = "complete")

#Make dendrogram
plot(data_clust_3, xlab="",ylab="Distance",main="Clustering for Stocks")

#identify three groups
rect.hclust(data_clust_3,k=5)
```

Plain dendogram for Model 3
```{r}
plot(data_clust_3,  main = "Plain dendrogram", hang = -1, cex = .5,
     xlab = "", ylab = "", sub = "", axes = FALSE)
```

##Model 4

Manhattan + Single
```{r}
#Get distance matrix
data_dist_4 <- dist(data_f[,-1], method = "manhattan")

#Perform cluster analysis
data_clust_4 <- hclust(data_dist_4, method = "single")

#Make dendrogram
plot(data_clust_4, xlab="",ylab="Distance",main="Clustering for Stocks")

#identify three groups
rect.hclust(data_clust_4, k=5)
```

Plain dendogram for Model 4
```{r}
plot(data_clust_4,  main = "Plain dendrogram", hang = -1, cex = .5,
     xlab = "", ylab = "", sub = "", axes = FALSE)
```

##Model 5

Euclidean + Average

```{r}
#Get distance matrix
data_dist_5 <- dist(data_f[,-1], method = "euclidean")

#Perform cluster analysis
data_clust_5 <- hclust(data_dist_5, method = "average")

#Make dendrogram
plot(data_clust_5, xlab="",ylab="Distance",main="Clustering for Stocks")

#identify three groups
rect.hclust(data_clust_5, k=5)
```

Plain dendogram for Model 5
```{r}
plot(data_clust_5,  main = "Plain dendrogram", hang = -1, cex = .5,
     xlab = "", ylab = "", sub = "", axes = FALSE)
```


##Model 6

Manhattan + Average

```{r}
#Get distance matrix
data_dist_6 <- dist(data_f[,-1], method = "euclidean")

#Perform cluster analysis
data_clust_6 <- hclust(data_dist_6, method = "average")

#Make dendrogram
plot(data_clust_6, xlab="",ylab="Distance", main="Clustering for Stocks")

#identify three groups
rect.hclust(data_clust_6, k=5)
```

Plain dendogram for Model 6
```{r}
plot(data_clust_6,  main = "Plain dendrogram", hang = -1, cex = .5,
     xlab = "", ylab = "", sub = "", axes = FALSE)
```



## PROBLEM 3
3.	If possible, run the SAS macro  or the R function to think about how many groups you want to retain (i.e. get the plots of cluster distance, R-squared, etc).  If you can’t run this, discuss how many groups you think are present.  

```{r}
source("http://reuningscherer.net/stat660/R/HClusEval.R.txt")

data_scaled_t <- t(data_f)
#get the distance matrix - canberra ignores entries where both are zero
# We utilise the Euclidian distance method to compute distances and intiate clustering. Although we are cognisant of the succeptibility of Euclidean distance to outliers, we note that our data does not appear to have significant outliers, after plotting boxplots of our variables.

dist1 <- dist(data_scaled_t, method="euclidean")

#now do clustering ??? use centroid method
clust1 <- hclust(dist1, method="ward.D")

# We draw the dendogram of our data and note that we have two rectangles, delineating the two clusters identified in our data.
plot(clust1, cex=.9, xlab="",ylab="Distance",main="Clustering for NYSE Data")
rect.hclust(clust1,k=2)
```

# We evaluate the number of groups that we wish to retain by calculating the 'guidance statistics,' namely: R-Squared, Root-mean-square standard deviation, semi-partial R-squared, and cluster distance. In computing these statistics and producing the plot, we find that
hclus_eval(data_scaled_t, dist_m = 'euclidean', clus_m = 'ward', plot_op = T)


## PROBLEM 4
4.	Run k-means clustering on your data.  Compare results to what you got in 3.)  Include a sum of squares vs. k (number of clusters) plot and comment on how many groups exist.

```{r}
#kdata is just normalized input dataset
kdata <- data_f
#set max value for number of clusters k
n.lev <- 15
# Calculate the within groups sum of squared error (SSE) for the number of cluster solutions selected by the user
wss <- rnorm(10)
while (prod(wss==sort(wss,decreasing=T))==0) {
  wss <- (nrow(kdata)-1)*sum(apply(kdata,2,var))
  for (i in 2:n.lev) wss[i] <- sum(kmeans(kdata, centers=i)$withinss)}
# Calculate the within groups SSE for 250 randomized data sets (based on the original input data)
k.rand <- function(x){
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}
rand.mat <- matrix(0,n.lev,250)
k.1 <- function(x) {
  for (i in 1:250) {
    r.mat <- as.matrix(suppressWarnings(k.rand(kdata)))
    rand.mat[,i] <- r.mat}
  return(rand.mat)
}
# Same function as above for data with < 3 column variables
k.2.rand <- function(x){
  rand.mat <- matrix(0,n.lev,250)
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}
k.2 <- function(x){
  for (i in 1:250) {
    r.1 <- k.2.rand(kdata)
    rand.mat[,i] <- r.1}
  return(rand.mat)
}
# Determine if the data data table has > or < 3 variables and call appropriate function above
if (dim(kdata)[2] == 2) { rand.mat <- k.2(kdata) } else { rand.mat <- k.1(kdata) }
# Plot within groups SSE against all tested cluster solutions for actual and randomized data - 1st: Log scale, 2nd: Normal scale
xrange <- range(1:n.lev)
yrange <- range(log(rand.mat),log(wss))
plot(xrange,yrange, type='n', xlab='Cluster Solution', ylab='Log of Within Group SSE', main='Cluster Solutions against Log of SSE')
for (i in 1:250) lines(log(rand.mat[,i]),type='l',col='red')
lines(log(wss), type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)
```


#HOW MANY LEVELS DO YOU WANT? DECIDE QULITATIVELY IN THE PREVIOUS PARTS!!!

```{r}
clust.level <- 7
# Apply K-means cluster solutions - append clusters to CSV file
fit <- kmeans(na.omit(kdata), clust.level)
aggregate(kdata, by=list(fit$cluster), FUN=mean)
```

```{r}
clust.out <- fit$cluster
kclust <- as.matrix(clust.out)
kclust.out <- cbind(kclust, data_f)
write.table(kclust.out, file="kmeans_out.csv", sep=",")
# Display Principal Components plot of data with clusters identified
clusplot(kdata, fit$cluster, shade=F, labels=2, lines=0, color=T, lty=4, main='Principal Components plot showing K-means clusters')
```


```{r}
plotcluster(kdata, fit$cluster, main="Five Cluster Solution in DA Space",
            xlab="First Discriminant Function", ylab="Second Discriminant Function")
```

## PROBLEM 5
5.	Comment on the number of groups that seem to be present based on what you find above.

