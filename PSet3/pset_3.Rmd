---
title: "PSet 4"
author: "Liana Wang"
date: "2/21/2019"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(aplpack)
library(fpc)
library(cluster)
library(ape)
library(tidyverse)
```

## PROBLEM 1
1.	Think about what metrics are appropriate for your data based on data type. Write a few sentences about this.  Also think about whether you should standardize or transform your data (comment as appropriate).


## PROBLEM 2
2.	Try various forms of hierarchical cluster analysis.  Try at least two different metrics and two agglomeration methods.  Produce dendrograms and comment on what you observe.
```{r}
new_data <- read.csv("https://pastebin.com/raw/bNrAceXf")
colnames(new_data)
```

##Model 1
```{r}
#Selecting Relevant Columns
index3 <- c(5, 9, 19, 20, 21, 26)
#Selecting Rows
rowindex <- seq(1, 89, by = 2)
data_1 <- new_data[rowindex,]
data_1 <- data_1 %>% remove_rownames %>% column_to_rownames(var="Ticker.Symbol")
#Scale the data
data_2 <- data_1[,index3]
data_f <- scale(data_2)
```
CONTINUOUS VARIABLES SO I WILL USE EUCLIDEAN DISTANCE

##Model 1
Euclidean Distance + Complete
```{r}
#Get distance matrix
data_dist <- dist(data_f[,-1], method="euclidean")
#Perform cluster analysis
data_clust <- hclust(data_dist, method = "complete")
#Make dendrogram
plot(data_clust, xlab="",ylab="Distance",main="Clustering for Stocks")
#identify three groups
rect.hclust(data_clust,k=5)
```

Plain dendogram
```{r}
plot(data_clust,  main = "Plain dendrogram", hang = -1, cex = 1.5,
     xlab = "", ylab = "", sub = "", axes = FALSE)
```

##Model 2

```{r}
#Get distance matrix
data_dist_2 <- dist(data_f[,-1], method = "euclidean")
#Perform cluster analysis
data_clust_2 <- hclust(data_dist_2, method = "single")
#Make dendrogram
plot(data_clust_2, xlab="",ylab="Distance",main="Clustering for Stocks")
#identify three groups
rect.hclust(data_clust_2, k=5)
```

Plain dendogram for Model 2
```{r}
plot(data_clust_2,  main = "Plain dendrogram", hang = -1, cex = 1.5,
     xlab = "", ylab = "", sub = "", axes = FALSE)
```

##Model 3

Manhattan + Complete
```{r}
#Get distance matrix
data_dist_3 <- dist(data_f[,-1], method = "manhattan")
#Perform cluster analysis
data_clust_3 <- hclust(data_dist_3, method = "complete")
#Make dendrogram
plot(data_clust_3, xlab="",ylab="Distance",main="Clustering for Stocks")
#identify three groups
rect.hclust(data_clust_3,k=5)
```

Plain dendogram for Model 3
```{r}
plot(data_clust_3,  main = "Plain dendrogram", hang = -1, cex = 1.5,
     xlab = "", ylab = "", sub = "", axes = FALSE)
```

##Model 4

Manhattan + Single
```{r}
#Get distance matrix
data_dist_4 <- dist(data_f[,-1], method = "manhattan")
#Perform cluster analysis
data_clust_4 <- hclust(data_dist_4, method = "single")
#Make dendrogram
plot(data_clust_4, xlab="",ylab="Distance",main="Clustering for Stocks")
#identify three groups
rect.hclust(data_clust_4, k=5)
```

Plain dendogram for Model 4
```{r}
plot(data_clust_4,  main = "Plain dendrogram", hang = -1, cex = 1.5,
     xlab = "", ylab = "", sub = "", axes = FALSE)
```



## PROBLEM 3
3.	If possible, run the SAS macro  or the R function to think about how many groups you want to retain (i.e. get the plots of cluster distance, R-squared, etc).  If you canâ€™t run this, discuss how many groups you think are present.  

```{r}
source("http://reuningscherer.net/stat660/R/HClusEval.R.txt")
data_scaled_t <- t(data_f)
#get the distance matrix - canberra ignores entries where both are zero
dist1 <- dist(data_scaled_t, method="minkowski", p=3)
#now do clustering ??? use centroid method
clust1 <- hclust(dist1, method="ward.D")
plot(clust1, cex=.9, xlab="",ylab="Distance",main="Clustering for NYSE Data")
rect.hclust(clust1,k=2)
```

```{r}
hclus_eval(data_scaled_t, dist_m = 'euclidean', clus_m = 'ward', plot_op = T)
```


## PROBLEM 4
4.	Run k-means clustering on your data.  Compare results to what you got in 3.)  Include a sum of squares vs. k (number of clusters) plot and comment on how many groups exist.

```{r}
#kdata is just normalized input dataset
kdata <- data_f
#set max value for number of clusters k
n.lev <- 15
# Calculate the within groups sum of squared error (SSE) for the number of cluster solutions selected by the user
wss <- rnorm(10)
while (prod(wss==sort(wss,decreasing=T))==0) {
  wss <- (nrow(kdata)-1)*sum(apply(kdata,2,var))
  for (i in 2:n.lev) wss[i] <- sum(kmeans(kdata, centers=i)$withinss)}
# Calculate the within groups SSE for 250 randomized data sets (based on the original input data)
k.rand <- function(x){
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}
rand.mat <- matrix(0,n.lev,250)
k.1 <- function(x) {
  for (i in 1:250) {
    r.mat <- as.matrix(suppressWarnings(k.rand(kdata)))
    rand.mat[,i] <- r.mat}
  return(rand.mat)
}
# Same function as above for data with < 3 column variables
k.2.rand <- function(x){
  rand.mat <- matrix(0,n.lev,250)
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}
k.2 <- function(x){
  for (i in 1:250) {
    r.1 <- k.2.rand(kdata)
    rand.mat[,i] <- r.1}
  return(rand.mat)
}
# Determine if the data data table has > or < 3 variables and call appropriate function above
if (dim(kdata)[2] == 2) { rand.mat <- k.2(kdata) } else { rand.mat <- k.1(kdata) }
# Plot within groups SSE against all tested cluster solutions for actual and randomized data - 1st: Log scale, 2nd: Normal scale
xrange <- range(1:n.lev)
yrange <- range(log(rand.mat),log(wss))
plot(xrange,yrange, type='n', xlab='Cluster Solution', ylab='Log of Within Group SSE', main='Cluster Solutions against Log of SSE')
for (i in 1:250) lines(log(rand.mat[,i]),type='l',col='red')
lines(log(wss), type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)
```


#HOW MANY LEVELS DO YOU WANT? DECIDE QULITATIVELY IN THE PREVIOUS PARTS!!!

```{r}
clust.level <- 7
# Apply K-means cluster solutions - append clusters to CSV file
fit <- kmeans(na.omit(kdata), clust.level)
aggregate(kdata, by=list(fit$cluster), FUN=mean)
```

```{r}
clust.out <- fit$cluster
kclust <- as.matrix(clust.out)
kclust.out <- cbind(kclust, data_f)
write.table(kclust.out, file="kmeans_out.csv", sep=",")
# Display Principal Components plot of data with clusters identified
clusplot(kdata, fit$cluster, shade=F, labels=2, lines=0, color=T, lty=4, main='Principal Components plot showing K-means clusters')
```


```{r}
plotcluster(kdata, fit$cluster, main="Five Cluster Solution in DA Space",
            xlab="First Discriminant Function", ylab="Second Discriminant Function")
```

## PROBLEM 5
5.	Comment on the number of groups that seem to be present based on what you find above.
