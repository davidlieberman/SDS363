---
title: 'S&DS363 Final Project'
author: "Yavuz Ramiz Ã‡olak, Ryo Tamaki, Liana Wang, David Lieberman"
date: "March 27th, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
rm(list=ls())
options(warn=-1)
library(knitr)
library(ggplot2)
library(ggfortify)
library(cowplot)
library(tidyr)
library(tidyverse)
library(MASS)
library(car)
library(class)
library(corrplot)
library(PerformanceAnalytics)
library(biotools)
library(DiscriMiner)
library(klaR)
library(e1071)
library(kernlab)
library(sparr)
library(spatstat)
library(mvtnorm)
library(mvoutlier)
library(ellipse)
library(data.table)
library(kableExtra)
```

IMPORTING OUR WORKING DATA
```{r}
working_data <- read.csv("https://pastebin.com/raw/BZWF6j97")
working_data <- as.data.frame(working_data[-1])
setattr(working_data, "row.names", as.vector(working_data$ticker))
working_data
```

## Introduction, Design and Primary Questions

### PLEASE EDIT MY WRITING HERE -- ADD TO THE INTRO, MY KNOWLEDGE OF ECON IS LIMITED
Our dataset follows S&P500 companies with 16 different variables quantifying their performance on the New York Stock Exchange as of 2017-12-31 (obtained via the Quandl API). Each row is a given a company, with column metrics ranging from Gross Profit and ROE, to Assets and Liabilities. **This report intends to identify various paterns within the data to predict {...}** In particular, we will use the following statistical techniques:

--Which performance metrics tend to vary greatly between stocks? Principal Component Analysis will yield a few uncorrelated principal components in the directions of greatest variance. This will allow us to see which performance metrics tend to correlate with one another (their weights within principal components), and reduce the overall dimensionality and effectively eliminate any correlation between variables to avoid any bias that would be introduced by multicollinearity.

--Are there particular performance metrics that can be used to decern which Sector a stock belongs to? Discriminant Analysis will allow us to see which principal components (and by extension the performance metrics contained in their loading coefficients) are most important for discriminanting between companies that did or did not outperform the SP500 during that same timeframe.

--THIRD THING

### PLEASE TALK MORE ABOUT WHAT ECON VARIABLES MEAN
## Data

Our dataset has:

--Categorical Variables (3): 'Ticker Symbol,' 'S&P500 Sector,' and 'Returns' (how much did the stock price percent change with respect to the S&P500 percent change within the same timeframe?)

--Continuous Variables (13): ...

```{r}
QQPlot <- function(x, na.rm = TRUE){
  plots <- list()
  j <- 1
  for (i in names(x)) {
    plots[[i]] <- ggplot(x, aes_string(sample = i)) + stat_qq() + stat_qq_line() + xlab(names(x)[j]) + ylab("")
    j <- j+1
  }
  print(plot_grid(plotlist = plots[1:9]))
  print(plot_grid(plotlist = plots[10:18]))
}
```


First, let's take a look at the data and make quantile-quantile plots for each performance metric to see if each variable is approximately univariate Normal.
```{r}
kable(working_data[1:10,]) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 6)
QQPlot(working_data[,-c(1:2,14:16)])
```

Several of these variables seem to deviate substantially from univariate Normality. Applying a log transformation will likely help. There are several subtleties. Many variables take on negative values, so to remedy this, I will first shift the data by the magnitude of the smallest value + 1 before taking its log. However, this addition disrupts ratio quantities, which, although it might be helpful to transform, by in large they seem approximately Normal enough that we will leave them unchanged.


TRANSFORM NUMERIC DATA AND REMOVE OUTLIERS
```{r, echo = TRUE, warning=FALSE}
data_transformed <- working_data[,-c(1:2,14:16)]

data_transformed$cashneq_liabilities <- log(data_transformed$cashneq_liabilities)

which(is.infinite(as.matrix(data_transformed)) == TRUE)
which(is.na(as.matrix(data_transformed)) == TRUE)

outliers <- as.vector(which(pcout(data_transformed, makeplot = FALSE)$wfinal01 == 0))
working_data <- working_data[-outliers,]
data_transformed <- data_transformed[-outliers,]

data_transformed
```

```{r}
QQPlot(data_transformed)
```

After log transforming the data, the variables all appear to be approximately univariate Normal (save for a few pesky outliers).

```{r}
CSQPlot<-function(vars,label="Chi-Square Quantile Plot"){
   #usually, vars is xxx$residuals or data from one group and label is for plot
     x<-cov(scale(vars),use="pairwise.complete.obs")
     squares<-sort(diag(as.matrix(scale(vars))%*%solve(x, tol=1e-20)%*%as.matrix(t(scale(vars)))))
     quantiles<-quantile(squares)
     hspr<-quantiles[4]-quantiles[2]
     cumprob<-c(1:length(vars[,1]))/length(vars[,1])-1/(2*length(vars[,1]))
     degf<-dim(x)[1]
     quants<-qchisq(cumprob,df=degf)
     gval<-(quants**(-1+degf/2))/(exp(quants/2)*gamma(degf/2)*(sqrt(2)**degf))
     scale<-hspr / (qchisq(.75,degf)-qchisq(.25,degf))
     se<-(scale/gval)*sqrt(cumprob*(1-cumprob)/length(squares))
     lower<-quants-2*se
     upper<-quants+2*se
    plot(quants,squares,col='red',pch=19,cex=1.2,xlab="Chi-Square Quantiles",
     ylab=label,main=paste("Chi-Square Quantiles for",label),ylim=range(upper,lower, squares) , xlim=range(c(0,quants)))
    lines(c(0,100),c(0,100),col=1)
    lines(quants,upper,col="blue",lty=2,lwd=2)
    lines(quants,lower,col="blue",lty=2,lwd=2)
    legend(0,range(upper,lower)[2]*.9,c("Data","95% Conf Limits"),lty=c(0,2),col=c("red","blue"),lwd=c(2,2),
      pch=c(19,NA))
}
```

Now, let's see if our data is approximately multivariate Normal using a chi-square quantile-quantile plot and seeing if the data all fall within the 95% confidence interval boundaries, denoted by the blue dotted lines on the graph below.
```{r}
CSQPlot(data_transformed, label="data_transformed")
```

As we can see, our data is very far from multivariate Normal -- nearly all of the data lie outside the 95% confidence limits on the chi-square quantile-quantile plot.


## Section 1: Principal Components Analysis

Let's construct a correlation matrix for our data. Hopefully, some variables will be highly correlated so our data will be a good match for Principal Component Analysis.
```{r}
correlation_matrix <- setattr(as.data.frame(round(cor(data_transformed), 2)), "row.names", names(data_transformed))
kable(correlation_matrix) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), font_size = 11)
```


Now we sort the correlations by largest magnitude and display the first six entries.
```{r}
correlation_vector <- as.vector(as.matrix(correlation_matrix))
correlation_vector <- correlation_vector[-as.vector(which(correlation_vector == 1))]
sorted_correlations <- correlation_vector[order(-abs(correlation_vector))][c(FALSE, TRUE)]
head(sorted_correlations)
```

Nice, we have some highly correlated variables. PCA will work well for our data.



Here's a more graphical representation of the correlation matrix that was created above.
```{r}
new_cor <- as.matrix(correlation_matrix)
for (i in 1:ncol(new_cor)){
  new_cor[i,i] <- 0
}

corrplot(new_cor, method = "color", tl.cex=0.6)
```

```{r}
chart.Correlation(data_transformed, histogram=FALSE)
```

```{r}
pc1 <- princomp(data_transformed, cor=TRUE)
pc2 <- prcomp(data_transformed, scale=TRUE)
```

```{r}
print(summary(pc2),digits=2)
```

According to the "eigenvalues greater than 1" rule, we will be keeping the first three principal components.

```{r}
loadings <- as.data.frame(pc1$loadings[,1:3])
kable(loadings) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Looking at the loading coefficients for the first three principal components ("major contributor":= abs(loading coefficient) > 0.3):

\\ NEED MORE ANALYSIS HERE

--First Principal Component Major Contributors: 

*It seems that all of these variables deal with ...* 


--Second Principal Component Major Contributors: 

*It seems that all of these variables deal with ...* 


--Third Principal Component Major Contributors: 

*It seems that all of these variables deal with ...*


```{r}
screeplot(pc2,type="lines",col="red",lwd=2,pch=19,cex=1.2,main="Scree Plot")
```
As we can see from the scree plot above, "cutting above the first elbow" leaves us with three principal components and is in good agreement with the "eigenvalues greater than one" findings above.

\\ NEED MORE ANALYSIS HERE

```{r}
ciscoreplot<-function(x,comps,namevec){
  y1<-sqrt(5.99*(x$sdev[comps[1]]^2))
  ymod<-y1-y1%%.05
  y1vec<-c(-y1,seq(-ymod,ymod,by=0.05),y1)
  y2vecpos<-sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
  y2vecneg<--sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
  y2vecpos[1]<-0
  y2vecneg[1]<-0
  y2vecpos[length(y2vecpos)]<-0
  y2vecneg[length(y2vecneg)]<-0
  plot(x$scores[,comps[1]],x$scores[,comps[2]],pch=19,cex=1.2,ylim=c(min(y2vecneg,x$scores[,comps[2]]),max(y2vecpos,x$scores[,comps[2]])),
    main="PC Score Plot", xlab=paste("Scores for PC",comps[1],sep=" "), ylab=paste("Scores for PC",comps[2],sep=" "),
    xlim=c(min(y1vec,x$scores[,comps[1]]),max(y1vec,x$scores[,comps[1]])))
    lines(y1vec,y2vecpos,col="Red",lwd=2)
    lines(y1vec,y2vecneg,col="Red",lwd=2)
  outliers <-((x$scores[,comps[1]]^2)/(x$sdev[comps[1]]^2)+(x$scores[,comps[2]]^2)/(x$sdev[comps[2]]^2))>5.99
  points(x$scores[outliers,comps[1]],x$scores[outliers,comps[2]],pch=19,cex=1.2,col="Blue")
  text(x$scores[outliers,comps[1]],x$scores[outliers,comps[2]],col="Blue",lab=namevec[outliers], pos=4)
}
```


```{r}
ciscoreplot(pc1,c(1,2), working_data$ticker)
ciscoreplot(pc1,c(1,3), working_data$ticker)
ciscoreplot(pc1,c(2,3), working_data$ticker)
```

\\ NEED MORE ANALYSIS HERE


```{r}
biplot(pc2,choices=c(1,2),pc.biplot=T)
biplot(pc2,choices=c(1,3),pc.biplot=T)
biplot(pc2,choices=c(2,3),pc.biplot=T)
```

\\ NEED MORE ANALYSIS HERE

```{r}
PCA_Data <- pc2$x[,c(1:3)]
Sectors <- droplevels(working_data$sector)
Returns <- working_data$returns

PCA_Data <- data.frame(Returns, Sectors, PCA_Data)

PCA_Data
```


\\ NEED MORE ANALYSIS HERE

## Section 2: Discriminant Analysis

Let's take a look at boxplots of each stock's principal component values, grouped by returns. "High" means the percent change of the stock price was more positive than the percent change of the SP500 between December 31st, 2017 and December 31st, 2018. "Low" means the opposite. We will be using the three principal components constructed above as our new working data, as they capture much of the variance of our original 11 continuous explanatory variables in only 3 dimensions and have the benefit of being uncorrelated.
```{r}
par(mar=c(3,5,2,2))
boxplot(PC1 ~ Returns, data=PCA_Data, horizontal = T, main="PC1 by Returns", las=2)
boxplot(PC2 ~ Returns, data=PCA_Data, horizontal = T, main="PC2 by Returns", las=2)
boxplot(PC3 ~ Returns, data=PCA_Data, horizontal = T, main="PC3 by Returns", las=2)
```

```{r}
plot(PCA_Data[,-c(1,2)], col = 2*(as.numeric(PCA_Data$Returns)), pch = as.numeric(PCA_Data$Returns)+15, cex=1.2)
```
Inspection of the principal components' projected scatters appears promising. All three groups (each has a different shape and color) seem to have a similar random scatter and "covariant footprint." In other words, it is a strong possibility that the data is multivariate Normal within groups, and the group covariance matricies are not significantly different from one another.

```{r}
CSQPlot(PCA_Data[which(PCA_Data$Returns == "Mediocre"),][-c(1,2)], label="Mediocre")
CSQPlot(PCA_Data[which(PCA_Data$Returns == "High"),][-c(1,2)], label="High")
CSQPlot(PCA_Data[which(PCA_Data$Returns == "Low"),][-c(1,2)], label="Low")
```

```{r, echo = TRUE}
print("Covariance Matrix for Mediocre")
cov(PCA_Data[PCA_Data$Returns == "Mediocre", 3:5])
cat(c("log-determinant", log(det(cov(PCA_Data[PCA_Data$Returns == "Mediocre", 3:5]))), "\n\n"))

print("Covariance Matrix for High")
cov(PCA_Data[PCA_Data$Returns == "High", 3:5])
cat(c("log-determinant", log(det(cov(PCA_Data[PCA_Data$Returns == "High", 3:5]))), "\n\n"))

print("Covariance Matrix for Low")
cov(PCA_Data[PCA_Data$Returns == "Low", 3:5])
cat(c("log-determinant", log(det(cov(PCA_Data[PCA_Data$Returns == "Low", 3:5]))), "\n\n"))

boxM(PCA_Data[,c("PC1","PC2","PC3")], PCA_Data$Returns)
```

STEPWISE LDA
```{r}
paste("PC1 correlation with percent_change:", cor(working_data$percent_change, PCA_Data$PC1))
paste("PC2 correlation with percent_change:", cor(working_data$percent_change, PCA_Data$PC2))
paste("PC3 correlation with percent_change:", cor(working_data$percent_change, PCA_Data$PC3)) #PC3 Biggest

data.LDA <- as.matrix(PCA_Data[,-c(1,2)]) # the data 
group.LDA <- as.factor(PCA_Data$Returns) # the classes 

# starting with PC3 because it has the highest pearson correlation with percent change

stepwise.lda <- stepclass(data.LDA, group.LDA, "lda", start.vars = "PC3")
stepwise.lda
plot(stepwise.lda)

# LMAO THIS WAS GARBAGE
```


LDA, no cross-validation
```{r}
LDA <- lda(Returns~PC1+PC2+PC3, PCA_Data, CV=FALSE) 
confusion <- table(PCA_Data$Returns, predict(LDA)$class)

print(confusion)
sum(diag(prop.table(confusion)))
```

QDA, with cross-validation
```{r}
cv.LDA <- lda(Returns~PC1+PC2+PC3, PCA_Data, CV=TRUE) 
confusion <- table(PCA_Data$Returns, cv.LDA$class)

print(confusion)
sum(diag(prop.table(confusion)))
```

```{r}
partimat(Returns~PC1+PC2+PC3, data=PCA_Data, method="lda")
```

```{r}
scores <- data.frame(Predicted_Returns=predict(LDA)$class, predict(LDA)$x, True_Returns=factor(PCA_Data$Returns))
LDA_classifications <- lda(scores[,2:3], scores[,1])

contour_grid <- expand.grid(LD1=seq(min(scores$LD1), max(scores$LD1), length.out=500), 
                            LD2=seq(min(scores$LD2), max(scores$LD2), length.out=500))
contour_prediction <- predict(LDA_classifications, newdata=contour_grid)
contour_data <- data.frame(contour_grid, z = as.numeric(contour_prediction$class))

ggplot(scores, aes(x=LD1, y=LD2)) + geom_point(size = 3, aes(pch=Predicted_Returns, col=Predicted_Returns)) + 
  stat_contour(aes(x = LD1, y = LD2, z = z), data = contour_data, size=1, color="black") +
  ggtitle("Predicted Returns") + theme(plot.title = element_text(size=20), legend.title=element_blank())

ggplot(scores, aes(x=LD1, y=LD2)) + geom_point(size = 3, aes(pch=True_Returns, col=True_Returns)) + 
  stat_contour(aes(x = LD1, y = LD2, z = z), data = contour_data, size=1, color="black") +
  ggtitle("True Returns") + theme(plot.title = element_text(size=20), legend.title=element_blank())
```


```{r}
posterior.LDA <- apply(predict(LDA)$posterior, 1, max)

correct_or_not <- as.factor(predict(LDA)$class == PCA_Data$Returns)

plot(x=seq(1,nrow(PCA_Data)), y=posterior.LDA, type="n", main="Posterior Probability of Predicted Membership Colored by Accuracy",
     xlab="Index",ylab="Posterior Probability")

points(which(correct_or_not == TRUE), y=posterior.LDA[correct_or_not == TRUE], col=3, pch=16, cex=1.25)
points(which(correct_or_not == FALSE), y=posterior.LDA[correct_or_not == FALSE], col=2, pch=15, cex=1.25)

legend("topright",legend=c("Correct", "Incorrect"), col=c(3,2), pch=c(15,16))
```

Nonparametric k-nearest neighbor (leave-one-out)
```{r}
results <- matrix(nrow = 45, ncol = 20)
for (j in 1:20) {
  for (i in 1:45) {
    test_point <- data.LDA[i,]
    train_data <- data.LDA[-i,]
    knn_prediction <- as.vector(knn(train=train_data, test=test_point, cl=PCA_Data$Returns[-i], k=j))
    truth <- as.vector(PCA_Data$Returns[i])
    results[i,j] <- truth == knn_prediction
  }
}

best_k <- which.max(colMeans(results))
best_k
success_rate <- colMeans(results)[best_k]
success_rate
```
Leaving out one point, and predicting its group using the k-nearest neighbor method, and repeating this for all 335 points in our dataset, this method predicts the group with approximately 46.7% success rate.

Support-Vector Machine, ML binary classification method, with cross-validation
```{r}
svm_data <- PCA_Data[,-2]

tune.out <- tune(svm, Returns~. , data=svm_data, kernel = "radial", cross = 30, scale = TRUE, ranges = list(cost = c(0.1,1,10), gamma = c(0.5,1,2)))
bestmod <- tune.out$best.model

bestmod$accuracies
cat("\n")
bestmod$tot.accuracy


# percent_correct <- c(); misclass <- matrix(rep(0, 9), nrow = 3, ncol = 3)
# for (i in 1:30){
#   withold <- as.vector(sample.int(nrow(svm_data), 30))
#   train <- svm_data[-withold,]
#   test <- svm_data[withold,]
# 
#   tune.out <- tune(svm, Returns~. , data=train, kernel = "radial", scale = TRUE, ranges = list(cost = c(0.1,1,10), gamma = c(0.5,1)))
#   bestmod <- tune.out$best.model
# 
#   predicted_returns.test <- predict(bestmod, test)
#   misclass.test <- table(predict = predicted_returns.test, truth = test$Returns)
#   percent_correct[i] <- sum(diag(prop.table(misclass.test)))
#   misclass <- misclass + as.matrix(misclass.test)
#   print(i)
# }
# 
# mean(percent_correct)
# misclass #out of 900
```

```{r}
svm_data$Returns <- ifelse(working_data$percent_change > sp500_percent, "High", "Low")
## ONLY TAKES BINARY HIGH/LOW


plot(ksvm(Returns~PC1+PC2, data=svm_data))
plot(ksvm(Returns~PC1+PC3, data=svm_data))
plot(ksvm(Returns~PC2+PC3, data=svm_data))
```


## Section 3: Cluster Analysis

Analyzing the question "will cutting at 10 clusters roughly group the companies into their respective sectors?"

```{r}
print("Summary Statistics for PC1")
paste("mean:", mean(PCA_Data$PC1))
paste("sd:", sd(PCA_Data$PC1))
invisible(qqPlot(PCA_Data$PC1))
cat("\n")

print("Summary Statistics for PC2")
paste("mean:", mean(PCA_Data$PC2))
paste("sd:", sd(PCA_Data$PC2))
invisible(qqPlot(PCA_Data$PC2))
cat("\n")

print("Summary Statistics for PC3")
paste("mean:", mean(PCA_Data$PC3))
paste("sd:", sd(PCA_Data$PC3))
invisible(qqPlot(PCA_Data$PC3))
cat("\n")
```
The summary statistics show that the PCA components appear to have approximately equal means and standard deviations and appear to be approximately univariate Normal, so we won't restandardize the data.

```{r}
data_dist <- dist(PCA_Data[,-c(1,2)], method = "euclidean")
data_clust <- hclust(data_dist, method = "ward.D2")

plot(data_clust, labels = FALSE, xlab="",ylab="Distance", main="Clustering for Stocks", cex = .5)
rect.hclust(data_clust, k=10) #10 Sectors
```

```{r}
cluster_groupings_companies <- as.numeric(cutree(data_clust, k=10))

cluster_dataframe <- data.frame(working_data[,c(1,2)],cluster_groupings_companies)
cluster_dataframe <- arrange(cluster_dataframe, factor(cluster_groupings_companies))
clusters <- cluster_dataframe %>% group_split(cluster_groupings_companies)

membership_proportion <- c(); cluster_analytics <- list(); classifications <- c()
for (i in 1:10){
  counts <- table(clusters[[i]]$sector)
  membership_proportion <- round(prop.table(counts)[which.max(counts)], 4)
  new_classifications <- ifelse(clusters[[i]]$sector == names(membership_proportion), TRUE, FALSE)
  names(new_classifications) <- clusters[[i]]$ticker
  cluster_analytics[[i]] <- c("membership_proportion"=as.character(membership_proportion), new_classifications)
  names(cluster_analytics)[i] <- paste(i,names(membership_proportion))
  classifications <- c(classifications, new_classifications)
}
classifications <- classifications[order(names(classifications))]

cluster_analytics

#classifications
cat("\n")
mean(classifications)
```


## Section 4: MANOVA

Let's take a look at boxplots of each stock's principal component values, grouped by industry.
```{r}
par(mar=c(3,15,2,2))
boxplot(PC1 ~ Sectors, data=PCA_Data, horizontal = T, main="PC1 by Industry", las=2)
boxplot(PC2 ~ Sectors, data=PCA_Data, horizontal = T, main="PC2 by Industry", las=2)
boxplot(PC3 ~ Sectors, data=PCA_Data, horizontal = T, main="PC3 by Industry", las=2)
```

```{r}
for (name in unique(Sectors)){
  CSQPlot(PCA_Data[which(Sectors==name),][-c(1:2)], label=name)
}
```


```{r}
manova <- manova(as.matrix(PCA_Data[-c(1:2)])~factor(PCA_Data$Sectors))
summary.manova(manova, test="Wilks")
```
