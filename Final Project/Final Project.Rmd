---
title: 'S&DS363 Final Project'
author: "Yavuz Ramiz Ã‡olak, Ryo Tamaki, Liana Wang, David Lieberman"
date: "March 10th, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
rm(list=ls())
options(warn=-1)
library(ggplot2)
library(ggfortify)
library(cowplot)
library(tidyr)
library(MASS)
library(corrplot)
library(mvtnorm)
library(ellipse)
library(data.table)
library(knitr)
library(kableExtra)
library(biotools)
library(DiscriMiner)
library(klaR)
```

```{r, include=FALSE}
data <- read.csv("https://pastebin.com/raw/Xzp5dCLG")
working_data <- data
working_data <- data[- as.vector(which(data$Sector %in% c("Financials", "Telecommunications Services"))),]
working_data <- working_data[,-c(3,5,9,11,15,19,20,22)]
setattr(working_data, "row.names", as.vector(working_data$Ticker.Symbol))
data_numeric <- working_data[,-c(1,2,21)]
```

## Introduction, Design and Primary Questions

### PLEASE EDIT MY WRITING HERE -- ADD TO THE INTRO, MY KNOWLEDGE OF ECON IS LIMITED
Our dataset follows 216 stocks on New York Stock Exchange with 21 different variables quantifying their performance on the S&P 500 between the market open on 2015-01-01 and the market close on 2015-12-31 (obtained from Kaggle). Each row contains the metrics for a given a company, with column metrics ranging from Earnings and Total Costs, to Profit Marging and Price/Earnings ratio. **This report intends to identify various paterns within the data:** In particular, we will use the following statistical techniques:

--Which performance metrics tend to vary greatly between stocks? Principal Component Analysis will yield a few uncorrelated principal components in the directions of greatest variance. This will allow us to see which performance metrics tend to correlate with one another (their weights within principal components), and reduce the dimensionality and correlation overall to a few "super variables" to be used in later analysis.

--Are there particular performance metrics that can be used to decern which Sector a stock belongs to? Discriminant Analysis will allow us to identify which principal components are best for discriminating between Sectors (ie. Do stocks in the Energy Sector normally have greater Total Assets than other Sectors), and create discriminator functions that can predict the Sector classification of an unknown stock given its performance metrics.

--THIRD THING

### PLEASE TALK MORE ABOUT WHAT ECON VARIABLES MEAN
## Data

Our dataset has:

--Categorical Variables (3): Ticker Symbol, Sector, Returns

--Discrete Variables (2): Current Ratio, Quick Ratio

--Continuous Variables (16): Capital Expenditures, Cash and Cash Equivalents, Depreciation, Gross Profit, Investments, Liabilities, Long Term Debt, Net Cash Flow, Net Income, Operating Income, Total Assets, Total Equity, Total Liabilities, Earnings Per Share, Estimated Shares Outstanding, Wide Percent Change

```{r}
QQPlot <- function(x, na.rm = TRUE){
  plots <- list()
  j <- 1
  for (i in names(x)) {
    plots[[i]] <- ggplot(x, aes_string(sample = i)) + stat_qq() + stat_qq_line() + xlab(names(x)[j]) + ylab("")
    j <- j+1
  }
  print(plot_grid(plotlist = plots[1:9]))
  print(plot_grid(plotlist = plots[10:18]))
}
```

First, let's take a look at the data and make quantile-quantile plots for each performance metric to see if each variable is approximately univariate Normal.
```{r}
kable(working_data[1:10,]) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 5)
QQPlot(data_numeric)
```

Several of these variables seem to deviate substantially from univariate Normality. Applying a log transformation will likely help. There are several subtleties. Many variables take on negative values, so to remedy this, I will first shift the data by the magnitude of the smallest value + 1 before taking its log. However, this addition disrupts ratio quantities. Thus, in the case of Earnings Per Share which contained negative values, we first take the absolute value, then apply the log transformation. Wide Percent Change already seemed approximately univariate Normal (approximately a straight line quantile-quantile plot), so it was not transformed.

### ASK JRDS ABOUT THESE TRANSFORMATIONS
```{r, echo = TRUE}
data_transformed <- data_numeric

data_transformed$Capital.Expenditures <- log(data_transformed$Capital.Expenditures + abs(min(data_transformed$Capital.Expenditures)) + 1)
data_transformed$Cash.and.Cash.Equivalents <- log(data_transformed$Cash.and.Cash.Equivalents)
data_transformed$Current.Ratio <- log(data_transformed$Current.Ratio)
data_transformed$Depreciation <- log(data_transformed$Depreciation + abs(min(data_transformed$Depreciation)) + 1)
data_transformed$Gross.Profit <- log(data_transformed$Gross.Profit + abs(min(data_transformed$Gross.Profit)) + 1)
data_transformed$Investments <- log(data_transformed$Investments + abs(min(data_transformed$Investments)) + 1)
data_transformed$Liabilities <- log(data_transformed$Liabilities + abs(min(data_transformed$Liabilities)) + 1)
data_transformed$Long.Term.Debt <- log(data_transformed$Long.Term.Debt + abs(min(data_transformed$Long.Term.Debt)) + 1)
data_transformed$Net.Cash.Flow <- log(data_transformed$Net.Cash.Flow + abs(min(data_transformed$Net.Cash.Flow)) + 1)
data_transformed$Net.Income <- log(data_transformed$Net.Income + abs(min(data_transformed$Net.Income)) + 1)
data_transformed$Operating.Income <- log(data_transformed$Operating.Income + abs(min(data_transformed$Operating.Income)) + 1)
data_transformed$Quick.Ratio <- log(data_transformed$Quick.Ratio)
data_transformed$Total.Assets <- log(data_transformed$Total.Assets)
data_transformed$Total.Equity <- log(data_transformed$Total.Equity + abs(min(data_transformed$Total.Equity)) + 1)
data_transformed$Total.Liabilities <- log(data_transformed$Total.Liabilities + abs(min(data_transformed$Total.Liabilities)) + 1)
data_transformed$Earnings.Per.Share <- log(abs(data_transformed$Earnings.Per.Share))
colnames(data_transformed)[colnames(data_transformed)=="Earnings.Per.Share"] <- "Abs.Earnings.Per.Share"
data_transformed$Estimated.Shares.Outstanding <- log(data_transformed$Estimated.Shares.Outstanding)
data_transformed$wide.percent_change <- data_transformed$wide.percent_change

data_transformed <- as.data.frame(data_transformed)
```


```{r}
QQPlot(data_transformed)
```

After log transforming the data, the variables all appear to be approximately univariate Normal (save for a few pesky outliers).

```{r}
CSQPlot<-function(vars,label="Chi-Square Quantile Plot"){
   #usually, vars is xxx$residuals or data from one group and label is for plot
     x<-cov(scale(vars),use="pairwise.complete.obs")
     squares<-sort(diag(as.matrix(scale(vars))%*%solve(x, tol=1e-20)%*%as.matrix(t(scale(vars)))))
     quantiles<-quantile(squares)
     hspr<-quantiles[4]-quantiles[2]
     cumprob<-c(1:length(vars[,1]))/length(vars[,1])-1/(2*length(vars[,1]))
     degf<-dim(x)[1]
     quants<-qchisq(cumprob,df=degf)
     gval<-(quants**(-1+degf/2))/(exp(quants/2)*gamma(degf/2)*(sqrt(2)**degf))
     scale<-hspr / (qchisq(.75,degf)-qchisq(.25,degf))
     se<-(scale/gval)*sqrt(cumprob*(1-cumprob)/length(squares))
     lower<-quants-2*se
     upper<-quants+2*se
    plot(quants,squares,col='red',pch=19,cex=1.2,xlab="Chi-Square Quantiles",
     ylab=label,main=paste("Chi-Square Quantiles for",label),ylim=range(upper,lower, squares) , xlim=range(c(0,quants)))
    lines(c(0,100),c(0,100),col=1)
    lines(quants,upper,col="blue",lty=2,lwd=2)
    lines(quants,lower,col="blue",lty=2,lwd=2)
    legend(0,range(upper,lower)[2]*.9,c("Data","95% Conf Limits"),lty=c(0,2),col=c("red","blue"),lwd=c(2,2),
      pch=c(19,NA))
}
```

Now, let's see if our data is approximately multivariate Normal using a chi-square quantile-quantile plot and seeing if the data all fall within the 95% confidence interval boundaries, denoted by the blue dotted lines on the graph below.
```{r}
CSQPlot(data_transformed, label="data_transformed")
```

As we can see, our data is very far from multivariate Normal -- nearly all of the data lie outside the 95% confidence limits on the chi-square quantile-quantile plot.


## Section 1: Principal Components Analysis

Let's construct a correlation matrix for our data. Hopefully, some variables will be highly correlated so our data will be a good match for Principal Component Analysis.
```{r}
correlation_matrix <- setattr(as.data.frame(round(cor(data_transformed), 2)), "row.names", names(data_transformed))
kable(correlation_matrix) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```


Now we sort the correlations by largest magnitude and display the first six entries.
```{r}
correlation_vector <- as.vector(as.matrix(correlation_matrix))
correlation_vector <- correlation_vector[-as.vector(which(correlation_vector == 1))]
sorted_correlations <- correlation_vector[order(-abs(correlation_vector))][c(FALSE, TRUE)]
head(sorted_correlations)
```

Nice, we have some highly correlated variables. PCA will work well for our data.



Here's a more graphical representation of the correlation matrix that was created above.
```{r}
correlation_plot <- cor(data_transformed)
for (i in 1:ncol(correlation_plot)){
  correlation_plot[i,i] <- 0
}
corrplot(correlation_plot, method = "color", tl.cex=0.6)
```

```{r}
pc1 <- princomp(data_transformed, cor=TRUE)
pc2 <- prcomp(data_transformed, scale=TRUE)
```

```{r}
print(summary(pc2),digits=2)
loadings <- as.data.frame(pc1$loadings[,1:4])
kable(loadings) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

The table above shows that if we want to account for at least 50% of the variance across our data, we should keep the first four principal components, which explain 51.8% of the variance total. Looking at the loading coefficients for the first four principal components ("major contributor":= abs(loading coefficient) > 0.3)

## ECON KNOWLEDGE PLEASE

--First Principal Component Major Contributors: Total.Liabilities (-0.491); Total.Assets (-0.487); Estimated.Shares.Outstanding (-0.401); Long.Term.Debt (-0.304)

*It seems that all of these variables deal with ...* 


--Second Principal Component Major Contributors: Quick.Ratio (-0.520); Current.Ratio (-0.517); Cash.and.Cash.Equivalents (-0.382); Operating.Income (0.360); Net.Income (0.358)

*It seems that all of these variables deal with ...* 

--Third Principal Component Major Contributors: Net.Income (-0.556); Operating.Income (-0.556)

*It seems that all of these variables deal with ...* 

--Fourth Principal Component Major Contributors: Wide.Percent_Change (0.618); Gross.Profit (0.443); Net.Cash.Flow (0.367); Total.Equity (-0.327)

*It seems that all of these variables deal with ...* 


```{r}
screeplot(pc2,type="lines",col="red",lwd=2,pch=19,cex=1.2,main="Scree Plot of Raw Drug Data")
```

As we can see from the scree plot above, successive components beyond the fourth principal component do little to explain the remaining variance of our data. According to the cut above the first "elbow" philosophy, we would only keep the first principal component. However, our priority is explaining at least 50% of the variance of these data, so we continue with our decision to retain the first four principal components.

```{r}
ciscoreplot<-function(x,comps,namevec){
  y1<-sqrt(5.99*(x$sdev[comps[1]]^2))
  ymod<-y1-y1%%.05
  y1vec<-c(-y1,seq(-ymod,ymod,by=0.05),y1)
  y2vecpos<-sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
  y2vecneg<--sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
  y2vecpos[1]<-0
  y2vecneg[1]<-0
  y2vecpos[length(y2vecpos)]<-0
  y2vecneg[length(y2vecneg)]<-0
  plot(x$scores[,comps[1]],x$scores[,comps[2]],pch=19,cex=1.2,ylim=c(min(y2vecneg,x$scores[,comps[2]]),max(y2vecpos,x$scores[,comps[2]])),
    main="PC Score Plot", xlab=paste("Scores for PC",comps[1],sep=" "), ylab=paste("Scores for PC",comps[2],sep=" "),
    xlim=c(min(y1vec,x$scores[,comps[1]]),max(y1vec,x$scores[,comps[1]])))
    lines(y1vec,y2vecpos,col="Red",lwd=2)
    lines(y1vec,y2vecneg,col="Red",lwd=2)
  outliers<-((x$scores[,comps[1]]^2)/(x$sdev[comps[1]]^2)+(x$scores[,comps[2]]^2)/(x$sdev[comps[2]]^2))>5.99
  points(x$scores[outliers,comps[1]],x$scores[outliers,comps[2]],pch=19,cex=1.2,col="Blue")
  text(x$scores[outliers,comps[1]],x$scores[outliers,comps[2]],col="Blue",lab=namevec[outliers], pos=4)
}
```


## GETTING SOME ERRORS HERE
```{r}
ciscoreplot(pc1,c(1,2), names(working_data))
ciscoreplot(pc1,c(1,3), names(working_data))
ciscoreplot(pc1,c(1,4), names(working_data))
```

## NEED MORE ANALYSIS HERE

Visualizing the score plot with a 95% confidence interval ellipse, there are some clear outliers. Let's take a look at the biplots.

```{r}
biplot(pc2,choices=c(1,2),pc.biplot=T)
biplot(pc2,choices=c(1,3),pc.biplot=T)
biplot(pc2,choices=c(1,4),pc.biplot=T)
```

The biplots corroborate what was seen in the loading coefficients table. The variables we identified as "major contibutors" for each principal component have large magnitude arrows pointing roughly along the axis of the respective principal components they are associated with. Additionally, we can see the ticker symbols of some major outliers that stray far from the tight clusters along certain principal component axes. 'APA' in PC2 and PC3; and 'PM,' 'XOM,' 'APC,' and 'FCX' in PC4.



Let's take a look at boxplots of each stock's principal component values, grouped by industry.

## SOMETHING WEIRD, NOT SEEING APA WITH A REALLY LOW PC2 SCORE AS SEEN IN THE BIPLOT
```{r}
varimax4 <- varimax(pc2$rotation[,1:4])
PCA_Data <- scale(as.matrix(data_transformed)) %*% varimax4$loadings
PCA_Data <- as.data.frame(PCA_Data)

Sectors <- as.vector(working_data$Sector)

PCA_Data <- cbind(Sectors, PCA_Data)
```

```{r}
par(mar=c(3,15,2,2))
boxplot(PC1 ~ Sectors, data=PCA_Data, horizontal = T, main="PC1 by Industry", las=2)
boxplot(PC2 ~ Sectors, data=PCA_Data, horizontal = T, main="PC2 by Industry", las=2)
boxplot(PC3 ~ Sectors, data=PCA_Data, horizontal = T, main="PC3 by Industry", las=2)
boxplot(PC4 ~ Sectors, data=PCA_Data, horizontal = T, main="PC4 by Industry", las=2)
```

## Section 2: Discriminant Analysis

## NEED TO ASK JRDS ABOUT THIS, DOESN"T MAKE TOO MUCH OF A DIFFERENCE, BUT WE MIGHT WANT TO CONSIDER IT
We saw we had some major outliers above, which will likely distrupt Discriminant Analysis going forward. I'm going to remove the large outliers from each principal component and then replot the boxplots to get a sense of the new outlier-excluded distributions.
```{r}
# PC1_outliers <- as.vector(which(PCA_Data$PC1 %in% as.vector(boxplot.stats(PCA_Data$PC1)$out)))
# PC2_outliers <- as.vector(which(PCA_Data$PC2 %in% as.vector(boxplot.stats(PCA_Data$PC2)$out)))
# PC3_outliers <- as.vector(which(PCA_Data$PC3 %in% as.vector(boxplot.stats(PCA_Data$PC3)$out)))
# PC4_outliers <- as.vector(which(PCA_Data$PC4 %in% as.vector(boxplot.stats(PCA_Data$PC4)$out)))
# PCA_Data <- PCA_Data[-c(PC1_outliers, PC2_outliers, PC3_outliers, PC4_outliers),]
# data_transformed <- data_transformed[-c(PC1_outliers, PC2_outliers, PC3_outliers, PC4_outliers),]
# working_data <- working_data[-c(PC1_outliers, PC2_outliers, PC3_outliers, PC4_outliers),]
# Sectors <- Sectors[-c(PC1_outliers, PC2_outliers, PC3_outliers, PC4_outliers)]
```

```{r}
# par(mar=c(3,15,2,2))
# boxplot(PC1 ~ Sectors, data=PCA_Data, horizontal = T, main="PC1 by Industry", las=2)
# boxplot(PC2 ~ Sectors, data=PCA_Data, horizontal = T, main="PC2 by Industry", las=2)
# boxplot(PC3 ~ Sectors, data=PCA_Data, horizontal = T, main="PC3 by Industry", las=2)
# boxplot(PC4 ~ Sectors, data=PCA_Data, horizontal = T, main="PC4 by Industry", las=2)
```
That looks a lot better.

```{r}
for (name in unique(Sectors)){
  print(paste("Covariance Matrix for", name))
  print(cov(PCA_Data[PCA_Data$Sector == name, 2:5]))
  cat(c("log-determinant", log(det(cov(PCA_Data[PCA_Data$Sector == name, 2:5]))), "\n\n"))
}

boxM(PCA_Data[,c("PC1","PC2","PC3","PC4")], PCA_Data$Sector)
```

```{r}
for (name in unique(Sectors)){
  CSQPlot(PCA_Data[which(Sectors==name),][-1], label=name)
}
```