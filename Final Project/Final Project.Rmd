---
title: 'S&DS363 Final Project'
author: "Yavuz Ramiz Ã‡olak, Ryo Tamaki, Liana Wang, David Lieberman"
date: "March 27th, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
rm(list=ls())
options(warn=-1)
library(knitr)
library(ggplot2)
library(ggfortify)
library(cowplot)
library(tidyr)
library(tidyverse)
library(MASS)
library(car)
library(class)
library(corrplot)
library(PerformanceAnalytics)
library(biotools)
library(DiscriMiner)
library(klaR)
library(e1071)
library(sparr)
library(spatstat)
library(mvtnorm)
library(mvoutlier)
library(ellipse)
library(data.table)
library(kableExtra)
library(Quandl)
```

DATA COLLECTION
```{r}
Quandl.api_key("LQ8syJ41h-Ar9UMq2M-N")
sp500 <- read.csv("https://pastebin.com/raw/ScRKxckW")
companies <- as.vector(as.character(sp500$Ticker))
sector <- as.vector(as.character(sp500$Sector))
data1 <- Quandl.datatable("SHARADAR/SF1",  ticker=companies[1:252], calendardate="2017-12-31")
data2 <- Quandl.datatable("SHARADAR/SF1",  ticker=companies[253:505], calendardate="2017-12-31")
data <- rbind(data1, data2)
data <- data[order(data$ticker),]

sector <- sector[companies %in% data$ticker]
data <- cbind(sector, data)
data <- data[,c(2,1,8:112)]
data <- data[ , apply(data, 2, function(x) !(length(which(is.na(x))) > 15))]
data <- na.omit(data)

# sink("outfile.txt")
# for (i in names(data)){
#   cat(i)
#   cat("\n")
# }
# sink()
```

```{r}
price_2018_1 <- Quandl.datatable("SHARADAR/SF1",  ticker=companies[1:252], calendardate="2018-12-31", qopts.columns=c("ticker", "price"))
price_2018_1 <- price_2018_1[order(price_2018_1$ticker),]
price_2018_2 <- Quandl.datatable("SHARADAR/SF1",  ticker=companies[253:505], calendardate="2018-12-31", qopts.columns=c("ticker", "price"))
price_2018_2 <- price_2018_2[order(price_2018_2$ticker),]
price_2018 <- rbind(price_2018_1, price_2018_2)


price_dataframe <- data.frame("ticker" = data$ticker, "price_2017" = data$price, "price_2018" = rep(NA, nrow(data)))
for (i in 1:nrow(data)){
  if (data$ticker[i] %in% price_2018$ticker){
      position_2018 <- which(price_2018$ticker == data$ticker[i])
      price_dataframe$price_2018[i] <- price_2018$price[position_2018]
  }
  else{
    price_dataframe$price_2018[i] <- NA
  }
}

missing <- as.vector(which(is.na(price_dataframe$price_2018)))
missing_ticker <- data$ticker[missing]
missing_prices <- c(NA, NA, 52.96, NA, 90.32, NA, NA, 53.20, NA, 171.82, 43.04, 92.36, 29.78, NA, NA, 92.95, NA, 83.20, NA, 44.74, NA, 244.84, 71.34, 93.15, NA)

j <- 1
for (i in missing){
  price_dataframe$price_2018[i] <- missing_prices[j]
  j <- j + 1
}

missing <- as.vector(which(is.na(price_dataframe$price_2018)))

data <- data[-missing,]
price_dataframe <- price_dataframe[-missing,]

percent_change <- (price_dataframe$price_2018 - price_dataframe$price_2017)/price_dataframe$price_2018
sp500_percent <- (2506.85 - 2673.61)/2506.85
lower_percent_bound <- sp500_percent + 1.5*sp500_percent
upper_percent_bound <- sp500_percent - 1.5*sp500_percent

returns <- c()
for (i in 1:length(percent_change)){
  if (between(percent_change[i], lower_percent_bound, upper_percent_bound, incbounds=TRUE)){
    returns[i] <- "Mediocre"
  }
  else if (percent_change[i] > upper_percent_bound){
    returns[i] <- "High"
  }
  else{
    returns[i] <- "Low"
  }
}

price_dataframe <- cbind(price_dataframe, percent_change)
price_dataframe <- cbind(price_dataframe, returns)
setattr(price_dataframe, "row.names", as.vector(data$ticker))

price_dataframe
```

```{r}
equity_assets <- data$equity / data$assets
cashneq_liabilities <- data$cashneq / data$liabilities
ebit_ev <- data$ebit / data$ev
ev_ebitda <- data$ev / data$ebitda

working_data <- data.frame("ticker"=data$ticker, "sector"=data$sector, cashneq_liabilities, equity_assets, ebit_ev, ev_ebitda, "ebitdamargin"=data$ebitdamargin, "grossmargin"=data$grossmargin, "netmargin"=data$netmargin, "roa"=data$roa, "roe"=data$roe, "roic"=data$roic, "ros"=data$ros, "ev"=data$ev, "percent_change"=price_dataframe$percent_change, "returns"=price_dataframe$returns)
setattr(working_data, "row.names", as.vector(working_data$ticker))


remove <- c(as.vector(which(working_data$sector %in% c("Telecommunication Services"))), 126)
working_data <- working_data[-remove,]

data_numeric <- working_data[,-c(1:2,14:16)]

working_data
```

## Introduction, Design and Primary Questions

### PLEASE EDIT MY WRITING HERE -- ADD TO THE INTRO, MY KNOWLEDGE OF ECON IS LIMITED
Our dataset follows S&P500 companies with 16 different variables quantifying their performance on the New York Stock Exchange as of 2017-12-31 (obtained via the Quandl API). Each row is a given a company, with column metrics ranging from Gross Profit and ROE, to Assets and Liabilities. **This report intends to identify various paterns within the data to predict {...}** In particular, we will use the following statistical techniques:

--Which performance metrics tend to vary greatly between stocks? Principal Component Analysis will yield a few uncorrelated principal components in the directions of greatest variance. This will allow us to see which performance metrics tend to correlate with one another (their weights within principal components), and reduce the overall dimensionality and effectively eliminate any correlation between variables to avoid any bias that would be introduced by multicollinearity.

--Are there particular performance metrics that can be used to decern which Sector a stock belongs to? Discriminant Analysis will allow us to see which principal components (and by extension the performance metrics contained in their loading coefficients) are most important for discriminanting between companies that did or did not outperform the SP500 during that same timeframe.

--THIRD THING

### PLEASE TALK MORE ABOUT WHAT ECON VARIABLES MEAN
## Data

Our dataset has:

--Categorical Variables (3): 'Ticker Symbol,' 'S&P500 Sector,' and 'Returns' (how much did the stock price percent change with respect to the S&P500 percent change within the same timeframe?)

--Continuous Variables (13): ...

```{r}
QQPlot <- function(x, na.rm = TRUE){
  plots <- list()
  j <- 1
  for (i in names(x)) {
    plots[[i]] <- ggplot(x, aes_string(sample = i)) + stat_qq() + stat_qq_line() + xlab(names(x)[j]) + ylab("")
    j <- j+1
  }
  print(plot_grid(plotlist = plots[1:9]))
  print(plot_grid(plotlist = plots[10:18]))
  print(plot_grid(plotlist = plots[19:27]))
}
```


First, let's take a look at the data and make quantile-quantile plots for each performance metric to see if each variable is approximately univariate Normal.
```{r}
kable(working_data[1:10,]) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 6)
QQPlot(data_numeric)
```

Several of these variables seem to deviate substantially from univariate Normality. Applying a log transformation will likely help. There are several subtleties. Many variables take on negative values, so to remedy this, I will first shift the data by the magnitude of the smallest value + 1 before taking its log. However, this addition disrupts ratio quantities, which, although it might be helpful to transform, by in large they seem approximately Normal enough that we will leave them unchanged.

### ASK JDRS ABOUT TRANSFORMATIONS AND OUTLIER TREATMENT
```{r, echo = TRUE, warning=FALSE}
data_transformed <- data_numeric

data_transformed$cashneq_liabilities <- log(data_transformed$cashneq_liabilities)

which(is.infinite(as.matrix(data_transformed)) == TRUE)
which(is.na(as.matrix(data_transformed)) == TRUE)

outliers <- as.vector(which(pcout(data_transformed, makeplot = FALSE)$wfinal01 == 0))
working_data <- working_data[-outliers,]
data_numeric <- data_numeric[-outliers,]
data_transformed <- data_transformed[-outliers,]

data_transformed
```



```{r}
QQPlot(data_transformed)
```

After log transforming the data, the variables all appear to be approximately univariate Normal (save for a few pesky outliers).

```{r}
CSQPlot<-function(vars,label="Chi-Square Quantile Plot"){
   #usually, vars is xxx$residuals or data from one group and label is for plot
     x<-cov(scale(vars),use="pairwise.complete.obs")
     squares<-sort(diag(as.matrix(scale(vars))%*%solve(x, tol=1e-20)%*%as.matrix(t(scale(vars)))))
     quantiles<-quantile(squares)
     hspr<-quantiles[4]-quantiles[2]
     cumprob<-c(1:length(vars[,1]))/length(vars[,1])-1/(2*length(vars[,1]))
     degf<-dim(x)[1]
     quants<-qchisq(cumprob,df=degf)
     gval<-(quants**(-1+degf/2))/(exp(quants/2)*gamma(degf/2)*(sqrt(2)**degf))
     scale<-hspr / (qchisq(.75,degf)-qchisq(.25,degf))
     se<-(scale/gval)*sqrt(cumprob*(1-cumprob)/length(squares))
     lower<-quants-2*se
     upper<-quants+2*se
    plot(quants,squares,col='red',pch=19,cex=1.2,xlab="Chi-Square Quantiles",
     ylab=label,main=paste("Chi-Square Quantiles for",label),ylim=range(upper,lower, squares) , xlim=range(c(0,quants)))
    lines(c(0,100),c(0,100),col=1)
    lines(quants,upper,col="blue",lty=2,lwd=2)
    lines(quants,lower,col="blue",lty=2,lwd=2)
    legend(0,range(upper,lower)[2]*.9,c("Data","95% Conf Limits"),lty=c(0,2),col=c("red","blue"),lwd=c(2,2),
      pch=c(19,NA))
}
```

Now, let's see if our data is approximately multivariate Normal using a chi-square quantile-quantile plot and seeing if the data all fall within the 95% confidence interval boundaries, denoted by the blue dotted lines on the graph below.
```{r}
CSQPlot(data_transformed, label="data_transformed")
```

As we can see, our data is very far from multivariate Normal -- nearly all of the data lie outside the 95% confidence limits on the chi-square quantile-quantile plot.


## Section 1: Principal Components Analysis

Let's construct a correlation matrix for our data. Hopefully, some variables will be highly correlated so our data will be a good match for Principal Component Analysis.
```{r}
correlation_matrix <- setattr(as.data.frame(round(cor(data_transformed), 2)), "row.names", names(data_transformed))
kable(correlation_matrix) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), font_size = 11)
```


Now we sort the correlations by largest magnitude and display the first six entries.
```{r}
correlation_vector <- as.vector(as.matrix(correlation_matrix))
correlation_vector <- correlation_vector[-as.vector(which(correlation_vector == 1))]
sorted_correlations <- correlation_vector[order(-abs(correlation_vector))][c(FALSE, TRUE)]
head(sorted_correlations)
```

Nice, we have some highly correlated variables. PCA will work well for our data.



Here's a more graphical representation of the correlation matrix that was created above.
```{r}
corrplot(cor(data_transformed), method = "color", order="AOE", tl.cex=0.6)
```

```{r}
chart.Correlation(data_transformed, histogram=FALSE)
```

```{r}
# variables <- data[-remove,]
# variables <- variables[-outliers,]
# 
# for (i in names(variables)[3:96]){
#   plot(variables[[i]], working_data$percent_change, xlab=i)
# }
```

```{r}
pc1 <- princomp(data_transformed, cor=TRUE)
pc2 <- prcomp(data_transformed, scale=TRUE)
```

```{r}
print(summary(pc2),digits=2)
```

According to the "eigenvalues greater than 1" rule, we will be keeping the first four principal components.

```{r}
loadings <- as.data.frame(pc1$loadings[,1:4])
kable(loadings) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Looking at the loading coefficients for the first four principal components ("major contributor":= abs(loading coefficient) > 0.3):

### ECON KNOWLEDGE PLEASE

--First Principal Component Major Contributors: 

*It seems that all of these variables deal with ...* 


--Second Principal Component Major Contributors: 

*It seems that all of these variables deal with ...* 

--Third Principal Component Major Contributors: 

*It seems that all of these variables deal with ...* 

--Fourth Principal Component Major Contributors: 

*It seems that all of these variables deal with ...* 


```{r}
screeplot(pc2,type="lines",col="red",lwd=2,pch=19,cex=1.2,main="Scree Plot")
```
As we can see from the scree plot above, successive components beyond the fourth principal component do little to explain the remaining variance of our data. It's hard to see exactly where to "cut above the first elbow" since the Scree plot seems relatively "smooth." However, in general, it seems to corroborate our previous decision to keep the first four principal components.

```{r}
ciscoreplot<-function(x,comps,namevec){
  y1<-sqrt(5.99*(x$sdev[comps[1]]^2))
  ymod<-y1-y1%%.05
  y1vec<-c(-y1,seq(-ymod,ymod,by=0.05),y1)
  y2vecpos<-sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
  y2vecneg<--sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
  y2vecpos[1]<-0
  y2vecneg[1]<-0
  y2vecpos[length(y2vecpos)]<-0
  y2vecneg[length(y2vecneg)]<-0
  plot(x$scores[,comps[1]],x$scores[,comps[2]],pch=19,cex=1.2,ylim=c(min(y2vecneg,x$scores[,comps[2]]),max(y2vecpos,x$scores[,comps[2]])),
    main="PC Score Plot", xlab=paste("Scores for PC",comps[1],sep=" "), ylab=paste("Scores for PC",comps[2],sep=" "),
    xlim=c(min(y1vec,x$scores[,comps[1]]),max(y1vec,x$scores[,comps[1]])))
    lines(y1vec,y2vecpos,col="Red",lwd=2)
    lines(y1vec,y2vecneg,col="Red",lwd=2)
  outliers <-((x$scores[,comps[1]]^2)/(x$sdev[comps[1]]^2)+(x$scores[,comps[2]]^2)/(x$sdev[comps[2]]^2))>5.99
  points(x$scores[outliers,comps[1]],x$scores[outliers,comps[2]],pch=19,cex=1.2,col="Blue")
  text(x$scores[outliers,comps[1]],x$scores[outliers,comps[2]],col="Blue",lab=namevec[outliers], pos=4)
  score_outliers <<- as.vector(which(outliers == TRUE))
}
```


```{r}
ciscoreplot(pc1,c(1,2), names(working_data))
# outliers <- score_outliers
# 
ciscoreplot(pc1,c(1,3), names(working_data))
# outliers <- c(outliers, score_outliers)
# 
ciscoreplot(pc1,c(1,4), names(working_data))
# outliers <- c(outliers, score_outliers)
# 
ciscoreplot(pc1,c(2,3), names(working_data))
# outliers <- c(outliers, score_outliers)
# 
ciscoreplot(pc1,c(2,4), names(working_data))
# outliers <- c(outliers, score_outliers)
# 
ciscoreplot(pc1,c(3,4), names(working_data))
# outliers <- c(outliers, score_outliers)
# 
# outliers <- unique(outliers)
```

\\ NEED MORE ANALYSIS HERE




```{r}
biplot(pc2,choices=c(1,2),pc.biplot=T)
biplot(pc2,choices=c(1,3),pc.biplot=T)
biplot(pc2,choices=c(1,4),pc.biplot=T)
biplot(pc2,choices=c(2,3),pc.biplot=T)
biplot(pc2,choices=c(2,4),pc.biplot=T)
biplot(pc2,choices=c(3,4),pc.biplot=T)
```

```{r}
# working_data <- working_data[-outliers,]
# data_transformed <- data_transformed[-outliers,]
# 
# pc3 <- prcomp(data_transformed, scale=TRUE)
# print(summary(pc3),digits=2)
```

```{r}
PCA_Data <- pc2$x[,c(1:4)]
Sectors <- droplevels(working_data$sector)
Returns <- working_data$returns

PCA_Data <- data.frame(Returns, Sectors, PCA_Data)

PCA_Data
```


Let's take a look at boxplots of each stock's principal component values, grouped by industry.
```{r}
par(mar=c(3,15,2,2))
boxplot(PC1 ~ Sectors, data=PCA_Data, horizontal = T, main="PC1 by Industry", las=2)
boxplot(PC2 ~ Sectors, data=PCA_Data, horizontal = T, main="PC2 by Industry", las=2)
boxplot(PC3 ~ Sectors, data=PCA_Data, horizontal = T, main="PC3 by Industry", las=2)
boxplot(PC4 ~ Sectors, data=PCA_Data, horizontal = T, main="PC4 by Industry", las=2)
```

## Section 2: Discriminant Analysis

Let's take a look at boxplots of each stock's principal component values, grouped by returns. "High" means the percent change of the stock price was more positive than the percent change of the SP500 between December 31st, 2017 and December 31st, 2018. "Low" means the opposite.
```{r}
par(mar=c(3,5,2,2))
boxplot(PC1 ~ Returns, data=PCA_Data, horizontal = T, main="PC1 by Returns", las=2)
boxplot(PC2 ~ Returns, data=PCA_Data, horizontal = T, main="PC2 by Returns", las=2)
boxplot(PC3 ~ Returns, data=PCA_Data, horizontal = T, main="PC3 by Returns", las=2)
boxplot(PC4 ~ Returns, data=PCA_Data, horizontal = T, main="PC4 by Returns", las=2)
```

```{r, echo = TRUE}
print("Covariance Matrix for Mediocre")
cov(PCA_Data[PCA_Data$Returns == "Mediocre", 3:6])
cat(c("log-determinant", log(det(cov(PCA_Data[PCA_Data$Returns == "Mediocre", 3:6]))), "\n\n"))

print("Covariance Matrix for High")
cov(PCA_Data[PCA_Data$Returns == "High", 3:6])
cat(c("log-determinant", log(det(cov(PCA_Data[PCA_Data$Returns == "High", 3:6]))), "\n\n"))

print("Covariance Matrix for Low")
cov(PCA_Data[PCA_Data$Returns == "Low", 3:6])
cat(c("log-determinant", log(det(cov(PCA_Data[PCA_Data$Returns == "Low", 3:6]))), "\n\n"))

boxM(PCA_Data[,c("PC1","PC2","PC3","PC4")], PCA_Data$Returns)
```

```{r}
CSQPlot(PCA_Data[which(working_data$returns == "Mediocre"),][-c(1,2)], label="Mediocre")
CSQPlot(PCA_Data[which(working_data$returns == "High"),][-c(1,2)], label="High")
CSQPlot(PCA_Data[which(working_data$returns == "Low"),][-c(1,2)], label="Low")
```

STEPWISE QDA
```{r}
correlations <- c(); j <- 1
for (i in names(data_transformed)){
  correlations[i] <- cor(working_data$percent_change, data_transformed[[i]])
}
names(data_transformed)[which.max(abs(correlations))] # roic

data.QDA <- data_transformed # the data 
group.QDA <- as.factor(working_data$returns) # the classes 

# starting with roic because it has the highest pearson correlation with percent change

stepwise.qda <- stepclass(data.QDA, group.QDA, "qda", start.vars = "roic")
stepwise.qda
plot(stepwise.qda)

# LMAO THIS WAS GARBAGE
```


```{r}
manova <- manova(as.matrix(data.QDA) ~ group.QDA)
summary.manova(manova,test="Wilks")
```

QDA, no cross-validation
```{r}
QDA <- qda(data.QDA, group.QDA, CV=FALSE) 
table(group.QDA, predict(QDA)$class)

# ~55.1% correct
```

QDA, with cross-validation
```{r}
cv.QDA <- qda(data.QDA, group.QDA, CV=TRUE) 
table(group.QDA, cv.QDA$class)

# ~46.1% correct
```



Nonparametric k-nearest neighbor (leave-one-out)
```{r}
results <- matrix(nrow = 45, ncol = 20)
for (j in 1:20) {
  for (i in 1:45) {
    test_point <- data.QDA[i,]
    train_data <- data.QDA[-i,]
    knn_prediction <- as.vector(knn(train = train_data, test = test_point, cl = group.QDA[-i], k = j))
    truth <- as.vector(group.QDA[i])
    results[i,j] <- truth == knn_prediction
  }
}

best_k <- which.max(colMeans(results))
best_k
success_rate <- colMeans(results)[best_k]
success_rate
```
Leaving out one point, and predicting its group using the k-nearest neighbor method, and repeating this for all 332 points in our dataset, this method predicts the group with approximately 46.7% success rate.

Support-Vector Machine, ML binary classification method, with cross-validation
```{r}
svm_data <- data.frame(data_transformed, "returns" = as.factor(working_data$returns))

# percent_correct <- c(); misclass <- matrix(rep(0, 9), nrow = 3, ncol = 3)
# for (i in 1:30){
#   withold <- as.vector(sample.int(nrow(svm_data), 30))
#   train <- svm_data[-withold,]
#   test <- svm_data[withold,]
# 
#   tune.out <- tune(svm, returns~. , data=train, kernel = "radial", scale = TRUE, ranges = list(cost = c(0.1,1,10), gamma = c(0.5,1)))
#   bestmod <- tune.out$best.model
# 
#   predicted_returns.test <- predict(bestmod, test)
#   misclass.test <- table(predict = predicted_returns.test, truth = test$returns)
#   percent_correct[i] <- sum(misclass.test[c(1,4,9)])/30
#   misclass <- misclass + as.matrix(misclass.test)
#   print(i)
# }
# 
# mean(percent_correct)
# misclass #out of 900
# 
# sum(misclass[c(1,5,9)])/900

tune.out <- tune(svm, returns~. , data=svm_data, kernel = "radial", cross = 30, scale = TRUE, ranges = list(cost = c(0.1,1,10), gamma = c(0.5,1,2)))
bestmod <- tune.out$best.model

bestmod$accuracies
cat("\n")
bestmod$tot.accuracy
```

