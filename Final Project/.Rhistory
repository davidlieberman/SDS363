price_dataframe
working_data <- data[,c(1:2,4,6,9,21,23,30,33,39:40,47,59,64,66,79,82:85)]
ev_ebitda <- data$ev / data$ebitda
working_data <- data.frame(working_data, ev_ebitda, "percent_change" = price_dataframe$percent_change, "returns" = price_dataframe$returns)
setattr(working_data, "row.names", as.vector(working_data$ticker))
working_data <- working_data[,c(1:7,21,8:20,22:23)]
remove <- c(as.vector(which(working_data$sector %in% c("Telecommunication Services"))),
as.vector(which(working_data$ebitda <= 0 )),
as.vector(which(working_data$ev_ebitda <= 0 )),
383,423,428)
working_data <- working_data[-remove,]
data_numeric <- working_data[,-c(1:2,22:23)]
working_data
test <- sign(working_data$netinc) * log(abs(working_data$netinc / 1e8))
QQPlot <- function(x, na.rm = TRUE){
plots <- list()
j <- 1
for (i in names(x)) {
plots[[i]] <- ggplot(x, aes_string(sample = i)) + stat_qq() + stat_qq_line() + xlab(names(x)[j]) + ylab("")
j <- j+1
}
print(plot_grid(plotlist = plots[1:9]))
print(plot_grid(plotlist = plots[10:18]))
print(plot_grid(plotlist = plots[19:27]))
}
kable(working_data[1:10,]) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 6)
QQPlot(data_numeric)
data_transformed <- data_numeric
data_transformed$assets <- log(data_transformed$assets)
data_transformed$assetturnover <- log(data_transformed$assetturnover)
data_transformed$cashneq <- log(data_transformed$cashneq + 1)
data_transformed$ebit <- log(data_transformed$ebit + abs(min(data_transformed$ebit)) + 1)
data_transformed$ev <- log(data_transformed$ev)
data_transformed$ev_ebitda <- log(data_transformed$ev_ebitda)
data_transformed$equity <- log(data_transformed$equity + abs(min(data_transformed$equity)) + 1)
data_transformed$gp <- log(data_transformed$gp + abs(min(data_transformed$gp)) + 1)
data_transformed$liabilities <- log(data_transformed$liabilities + abs(min(data_transformed$liabilities)) + 1)
data_transformed$netinc <- log(data_transformed$netinc + abs(min(data_transformed$netinc)) + 1)
data_transformed$opinc <- log(data_transformed$opinc + abs(min(data_transformed$opinc)) + 1)
data_transformed$revenue <- log(data_transformed$revenue)
which(is.infinite(as.matrix(data_transformed)) == TRUE)
which(is.na(as.matrix(data_transformed)) == TRUE)
data_transformed <- as.data.frame(data_transformed)
# outliers <- as.vector(which(pcout(data_transformed, makeplot = FALSE)$wfinal01 == 0))
# working_data <- working_data[-outliers,]
# data_numeric <- data_numeric[-outliers,]
# data_transformed <- data_transformed[-outliers,]
data_transformed
QQPlot(data_transformed)
CSQPlot<-function(vars,label="Chi-Square Quantile Plot"){
#usually, vars is xxx$residuals or data from one group and label is for plot
x<-cov(scale(vars),use="pairwise.complete.obs")
squares<-sort(diag(as.matrix(scale(vars))%*%solve(x, tol=1e-20)%*%as.matrix(t(scale(vars)))))
quantiles<-quantile(squares)
hspr<-quantiles[4]-quantiles[2]
cumprob<-c(1:length(vars[,1]))/length(vars[,1])-1/(2*length(vars[,1]))
degf<-dim(x)[1]
quants<-qchisq(cumprob,df=degf)
gval<-(quants**(-1+degf/2))/(exp(quants/2)*gamma(degf/2)*(sqrt(2)**degf))
scale<-hspr / (qchisq(.75,degf)-qchisq(.25,degf))
se<-(scale/gval)*sqrt(cumprob*(1-cumprob)/length(squares))
lower<-quants-2*se
upper<-quants+2*se
plot(quants,squares,col='red',pch=19,cex=1.2,xlab="Chi-Square Quantiles",
ylab=label,main=paste("Chi-Square Quantiles for",label),ylim=range(upper,lower, squares) , xlim=range(c(0,quants)))
lines(c(0,100),c(0,100),col=1)
lines(quants,upper,col="blue",lty=2,lwd=2)
lines(quants,lower,col="blue",lty=2,lwd=2)
legend(0,range(upper,lower)[2]*.9,c("Data","95% Conf Limits"),lty=c(0,2),col=c("red","blue"),lwd=c(2,2),
pch=c(19,NA))
}
CSQPlot(data_transformed, label="data_transformed")
correlation_matrix <- setattr(as.data.frame(round(cor(data_transformed), 2)), "row.names", names(data_transformed))
kable(correlation_matrix) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), font_size = 8)
correlation_vector <- as.vector(as.matrix(correlation_matrix))
correlation_vector <- correlation_vector[-as.vector(which(correlation_vector == 1))]
sorted_correlations <- correlation_vector[order(-abs(correlation_vector))][c(FALSE, TRUE)]
head(sorted_correlations)
corrplot(cor(data_transformed), method = "color", order="AOE", tl.cex=0.6)
chart.Correlation(data_transformed, histogram=FALSE)
# variables <- data[-remove,]
# variables <- variables[-outliers,]
#
# for (i in names(variables)[3:96]){
#   plot(variables[[i]], working_data$percent_change, xlab=i)
# }
pc1 <- princomp(data_transformed, cor=TRUE)
pc2 <- prcomp(data_transformed, scale=TRUE)
print(summary(pc2),digits=2)
loadings <- as.data.frame(pc1$loadings[,1:4])
kable(loadings) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
screeplot(pc2,type="lines",col="red",lwd=2,pch=19,cex=1.2,main="Scree Plot")
ciscoreplot<-function(x,comps,namevec){
y1<-sqrt(5.99*(x$sdev[comps[1]]^2))
ymod<-y1-y1%%.05
y1vec<-c(-y1,seq(-ymod,ymod,by=0.05),y1)
y2vecpos<-sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
y2vecneg<--sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
y2vecpos[1]<-0
y2vecneg[1]<-0
y2vecpos[length(y2vecpos)]<-0
y2vecneg[length(y2vecneg)]<-0
plot(x$scores[,comps[1]],x$scores[,comps[2]],pch=19,cex=1.2,ylim=c(min(y2vecneg,x$scores[,comps[2]]),max(y2vecpos,x$scores[,comps[2]])),
main="PC Score Plot", xlab=paste("Scores for PC",comps[1],sep=" "), ylab=paste("Scores for PC",comps[2],sep=" "),
xlim=c(min(y1vec,x$scores[,comps[1]]),max(y1vec,x$scores[,comps[1]])))
lines(y1vec,y2vecpos,col="Red",lwd=2)
lines(y1vec,y2vecneg,col="Red",lwd=2)
outliers<-((x$scores[,comps[1]]^2)/(x$sdev[comps[1]]^2)+(x$scores[,comps[2]]^2)/(x$sdev[comps[2]]^2))>5.99
points(x$scores[outliers,comps[1]],x$scores[outliers,comps[2]],pch=19,cex=1.2,col="Blue")
text(x$scores[outliers,comps[1]],x$scores[outliers,comps[2]],col="Blue",lab=namevec[outliers], pos=4)
}
ciscoreplot(pc1,c(1,2), names(working_data))
ciscoreplot(pc1,c(1,3), names(working_data))
ciscoreplot(pc1,c(1,4), names(working_data))
biplot(pc2,choices=c(1,2),pc.biplot=T)
biplot(pc2,choices=c(1,3),pc.biplot=T)
biplot(pc2,choices=c(1,4),pc.biplot=T)
autoplot(pc2, x=1, y=2)
autoplot(pc2, x=1, y=3)
autoplot(pc2, x=1, y=4)
PCA_Data <- pc2$x[,c(1:4)]
Sectors <- working_data$sector
Returns <- working_data$returns
PCA_Data <- data.frame(Returns, Sectors, PCA_Data)
PCA_Data
par(mar=c(3,15,2,2))
boxplot(PC1 ~ Sectors, data=PCA_Data, horizontal = T, main="PC1 by Industry", las=2)
boxplot(PC2 ~ Sectors, data=PCA_Data, horizontal = T, main="PC2 by Industry", las=2)
boxplot(PC3 ~ Sectors, data=PCA_Data, horizontal = T, main="PC3 by Industry", las=2)
boxplot(PC4 ~ Sectors, data=PCA_Data, horizontal = T, main="PC4 by Industry", las=2)
boxplot(PC1 ~ Returns, data=PCA_Data, horizontal = T, main="PC1 by Returns", las=2)
boxplot(PC2 ~ Returns, data=PCA_Data, horizontal = T, main="PC2 by Returns", las=2)
boxplot(PC3 ~ Returns, data=PCA_Data, horizontal = T, main="PC3 by Returns", las=2)
boxplot(PC4 ~ Returns, data=PCA_Data, horizontal = T, main="PC4 by Returns", las=2)
print("Covariance Matrix for High")
cov(PCA_Data[PCA_Data$Returns == "High", 3:6])
cat(c("log-determinant", log(det(cov(PCA_Data[PCA_Data$Returns == "High", 3:6]))), "\n\n"))
print("Covariance Matrix for Low")
cov(PCA_Data[PCA_Data$Returns == "Low", 3:6])
cat(c("log-determinant", log(det(cov(PCA_Data[PCA_Data$Returns == "Low", 3:6]))), "\n\n"))
boxM(PCA_Data[,c("PC1","PC2","PC3","PC4")], PCA_Data$Returns)
CSQPlot(PCA_Data[which(working_data$returns == "High"),][-c(1,2)], label="High")
CSQPlot(PCA_Data[which(working_data$returns == "Low"),][-c(1,2)], label="Low")
correlations <- c(); j <- 1
for (i in names(data_transformed)){
correlations[i] <- cor(working_data$percent_change, data_transformed[[i]])
}
names(data_transformed)[which.max(abs(correlations))] # roic
data.QDA <- data_transformed # the data
group.QDA <- working_data$returns # the classes
# starting with roic because it has the highest pearson correlation with percent change
stepwise.qda <- stepclass(data.QDA, group.QDA, "qda", start.vars = "roic")
stepwise.qda
plot(stepwise.qda)
# LMAO THIS WAS GARBAGE
manova <- manova(as.matrix(data.QDA) ~ group.QDA)
summary.aov(manova)
summary.manova(manova,test="Wilks")
QDA <- qda(data.QDA, group.QDA, CV=FALSE)
table(group.QDA, predict(QDA)$class)
# 72.5% correct
cv.QDA <- qda(data.QDA, group.QDA, CV=TRUE)
table(group.QDA, cv.QDA$class)
# ~62.2% correct
# partimat(as.factor(group.QDA)~. , data=data.QDA, method="qda")
# GETTING ERRORS NEED TO ASK JDRS
results <- matrix(nrow = 45, ncol = 20)
for (j in 1:20) {
for (i in 1:45) {
test_point <- data.QDA[i,]
train_data <- data.QDA[-i,]
knn_prediction <- as.vector(knn(train = train_data, test = test_point, cl = group.QDA[-i], k = j))
truth <- as.vector(group.QDA[i])
results[i,j] <- truth == knn_prediction
}
}
best_k <- which.max(colMeans(results))
best_k
success_rate <- colMeans(results)[best_k]
success_rate
svm_data <- data.frame(data_transformed, "returns" = as.factor(working_data$returns))
# percent_correct <- c()
# for (i in 1:30){
#   withold <- as.vector(sample.int(nrow(svm_data), 30))
#   train <- svm_data[-withold,]
#   test <- svm_data[withold,]
#
#   tune.out <- tune(svm, returns~. , data=train, kernel = "radial", scale = TRUE,ranges = list(cost = c(0.1,1,10), gamma = c(0.5,1)))
#   bestmod <- tune.out$best.model
#
#   predicted_returns.test <- predict(bestmod, test)
#   misclass.test <- table(predict = predicted_returns.test, truth = test$returns)
#   percent_correct[i] <- sum(misclass.test[c(1,4)])/30
#   print(i)
# }
#
# mean(percent_correct)
tune.out <- tune(svm, returns~. , data=svm_data, kernel = "radial", cross = 30, scale = TRUE, ranges = list(cost = c(0.1,1,10), gamma = c(0.5,1,2)))
bestmod <- tune.out$best.model
bestmod$accuracies
cat("\n")
bestmod$tot.accuracy
partimat(as.factor(group.QDA)~. , data=data.QDA, method="qda")
knitr::opts_chunk$set(echo = FALSE)
rm(list=ls())
options(warn=-1)
library(knitr)
library(ggplot2)
library(ggfortify)
library(cowplot)
library(tidyr)
library(tidyverse)
library(MASS)
library(car)
library(class)
library(corrplot)
library(PerformanceAnalytics)
library(biotools)
library(DiscriMiner)
library(klaR)
library(e1071)
library(sparr)
library(spatstat)
library(mvtnorm)
library(mvoutlier)
library(ellipse)
library(data.table)
library(kableExtra)
library(Quandl)
Quandl.api_key("LQ8syJ41h-Ar9UMq2M-N")
sp500 <- read.csv("https://pastebin.com/raw/ScRKxckW")
companies <- as.vector(as.character(sp500$Ticker))
sector <- as.vector(as.character(sp500$Sector))
data1 <- Quandl.datatable("SHARADAR/SF1",  ticker=companies[1:252], calendardate="2017-12-31")
data2 <- Quandl.datatable("SHARADAR/SF1",  ticker=companies[253:505], calendardate="2017-12-31")
data <- rbind(data1, data2)
data <- data[order(data$ticker),]
sector <- sector[companies %in% data$ticker]
data <- cbind(sector, data)
data <- data[,c(2,1,8:112)]
data <- data[ , apply(data, 2, function(x) !(length(which(is.na(x))) > 15))]
data <- na.omit(data)
# sink("outfile.txt")
# for (i in names(data)){
#   cat(i)
#   cat("\n")
# }
# sink()
price_2018_1 <- Quandl.datatable("SHARADAR/SF1",  ticker=companies[1:252], calendardate="2018-12-31", qopts.columns=c("ticker", "price"))
price_2018_1 <- price_2018_1[order(price_2018_1$ticker),]
price_2018_2 <- Quandl.datatable("SHARADAR/SF1",  ticker=companies[253:505], calendardate="2018-12-31", qopts.columns=c("ticker", "price"))
price_2018_2 <- price_2018_2[order(price_2018_2$ticker),]
price_2018 <- rbind(price_2018_1, price_2018_2)
price_dataframe <- data.frame("ticker" = data$ticker, "price_2017" = data$price, "price_2018" = rep(NA, nrow(data)))
for (i in 1:nrow(data)){
if (data$ticker[i] %in% price_2018$ticker){
position_2018 <- which(price_2018$ticker == data$ticker[i])
price_dataframe$price_2018[i] <- price_2018$price[position_2018]
}
else{
price_dataframe$price_2018[i] <- NA
}
}
missing <- as.vector(which(is.na(price_dataframe$price_2018)))
missing_ticker <- data$ticker[missing]
missing_prices <- c(NA, NA, 52.96, NA, 90.32, NA, NA, 53.20, NA, 171.82, 43.04, 92.36, 29.78, NA, NA, 92.95, NA, 83.20, NA, 44.74, NA, 244.84, 71.34, 93.15, NA)
j <- 1
for (i in missing){
price_dataframe$price_2018[i] <- missing_prices[j]
j <- j + 1
}
missing <- as.vector(which(is.na(price_dataframe$price_2018)))
data <- data[-missing,]
price_dataframe <- price_dataframe[-missing,]
percent_change <- (price_dataframe$price_2018 - price_dataframe$price_2017)/price_dataframe$price_2018
sp500_percent <- (2506.85 - 2673.61)/2506.85
lower_percent_bound <- sp500_percent + 1.5*sp500_percent
upper_percent_bound <- sp500_percent - 1.5*sp500_percent
returns <- c()
for (i in 1:length(percent_change)){
if (between(percent_change[i], lower_percent_bound, upper_percent_bound, incbounds=TRUE)){
returns[i] <- "Mediocre"
}
else if (percent_change[i] > upper_percent_bound){
returns[i] <- "High"
}
else{
returns[i] <- "Low"
}
}
price_dataframe <- cbind(price_dataframe, percent_change)
price_dataframe <- cbind(price_dataframe, returns)
setattr(price_dataframe, "row.names", as.vector(data$ticker))
price_dataframe
equity_assets <- data$equity / data$assets
cashneq_liabilities <- data$cashneq / data$liabilities
ebit_ev <- data$ebit / data$ev
ev_ebitda <- data$ev / data$ebitda
working_data <- data.frame("ticker"=data$ticker, "sector"=data$sector, cashneq_liabilities, equity_assets, ebit_ev, ev_ebitda, "ebitdamargin"=data$ebitdamargin, "grossmargin"=data$grossmargin, "netmargin"=data$netmargin, "roa"=data$roa, "roe"=data$roe, "roic"=data$roic, "ros"=data$ros, "ev"=data$ev, "percent_change"=price_dataframe$percent_change, "returns"=price_dataframe$returns)
setattr(working_data, "row.names", as.vector(working_data$ticker))
remove <- c(as.vector(which(working_data$sector %in% c("Telecommunication Services"))), 126)
working_data <- working_data[-remove,]
data_numeric <- working_data[,-c(1:2,14:16)]
working_data
QQPlot <- function(x, na.rm = TRUE){
plots <- list()
j <- 1
for (i in names(x)) {
plots[[i]] <- ggplot(x, aes_string(sample = i)) + stat_qq() + stat_qq_line() + xlab(names(x)[j]) + ylab("")
j <- j+1
}
print(plot_grid(plotlist = plots[1:9]))
print(plot_grid(plotlist = plots[10:18]))
print(plot_grid(plotlist = plots[19:27]))
}
kable(working_data[1:10,]) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 6)
QQPlot(data_numeric)
data_transformed <- data_numeric
data_transformed$cashneq_liabilities <- log(data_transformed$cashneq_liabilities)
which(is.infinite(as.matrix(data_transformed)) == TRUE)
which(is.na(as.matrix(data_transformed)) == TRUE)
outliers <- as.vector(which(pcout(data_transformed, makeplot = FALSE)$wfinal01 == 0))
working_data <- working_data[-outliers,]
data_numeric <- data_numeric[-outliers,]
data_transformed <- data_transformed[-outliers,]
data_transformed
QQPlot(data_transformed)
CSQPlot<-function(vars,label="Chi-Square Quantile Plot"){
#usually, vars is xxx$residuals or data from one group and label is for plot
x<-cov(scale(vars),use="pairwise.complete.obs")
squares<-sort(diag(as.matrix(scale(vars))%*%solve(x, tol=1e-20)%*%as.matrix(t(scale(vars)))))
quantiles<-quantile(squares)
hspr<-quantiles[4]-quantiles[2]
cumprob<-c(1:length(vars[,1]))/length(vars[,1])-1/(2*length(vars[,1]))
degf<-dim(x)[1]
quants<-qchisq(cumprob,df=degf)
gval<-(quants**(-1+degf/2))/(exp(quants/2)*gamma(degf/2)*(sqrt(2)**degf))
scale<-hspr / (qchisq(.75,degf)-qchisq(.25,degf))
se<-(scale/gval)*sqrt(cumprob*(1-cumprob)/length(squares))
lower<-quants-2*se
upper<-quants+2*se
plot(quants,squares,col='red',pch=19,cex=1.2,xlab="Chi-Square Quantiles",
ylab=label,main=paste("Chi-Square Quantiles for",label),ylim=range(upper,lower, squares) , xlim=range(c(0,quants)))
lines(c(0,100),c(0,100),col=1)
lines(quants,upper,col="blue",lty=2,lwd=2)
lines(quants,lower,col="blue",lty=2,lwd=2)
legend(0,range(upper,lower)[2]*.9,c("Data","95% Conf Limits"),lty=c(0,2),col=c("red","blue"),lwd=c(2,2),
pch=c(19,NA))
}
CSQPlot(data_transformed, label="data_transformed")
correlation_matrix <- setattr(as.data.frame(round(cor(data_transformed), 2)), "row.names", names(data_transformed))
kable(correlation_matrix) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), font_size = 11)
correlation_vector <- as.vector(as.matrix(correlation_matrix))
correlation_vector <- correlation_vector[-as.vector(which(correlation_vector == 1))]
sorted_correlations <- correlation_vector[order(-abs(correlation_vector))][c(FALSE, TRUE)]
head(sorted_correlations)
corrplot(cor(data_transformed), method = "color", order="AOE", tl.cex=0.6)
chart.Correlation(data_transformed, histogram=FALSE)
# variables <- data[-remove,]
# variables <- variables[-outliers,]
#
# for (i in names(variables)[3:96]){
#   plot(variables[[i]], working_data$percent_change, xlab=i)
# }
pc1 <- princomp(data_transformed, cor=TRUE)
pc2 <- prcomp(data_transformed, scale=TRUE)
print(summary(pc2),digits=2)
loadings <- as.data.frame(pc1$loadings[,1:4])
kable(loadings) %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
screeplot(pc2,type="lines",col="red",lwd=2,pch=19,cex=1.2,main="Scree Plot")
ciscoreplot<-function(x,comps,namevec){
y1<-sqrt(5.99*(x$sdev[comps[1]]^2))
ymod<-y1-y1%%.05
y1vec<-c(-y1,seq(-ymod,ymod,by=0.05),y1)
y2vecpos<-sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
y2vecneg<--sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
y2vecpos[1]<-0
y2vecneg[1]<-0
y2vecpos[length(y2vecpos)]<-0
y2vecneg[length(y2vecneg)]<-0
plot(x$scores[,comps[1]],x$scores[,comps[2]],pch=19,cex=1.2,ylim=c(min(y2vecneg,x$scores[,comps[2]]),max(y2vecpos,x$scores[,comps[2]])),
main="PC Score Plot", xlab=paste("Scores for PC",comps[1],sep=" "), ylab=paste("Scores for PC",comps[2],sep=" "),
xlim=c(min(y1vec,x$scores[,comps[1]]),max(y1vec,x$scores[,comps[1]])))
lines(y1vec,y2vecpos,col="Red",lwd=2)
lines(y1vec,y2vecneg,col="Red",lwd=2)
outliers <-((x$scores[,comps[1]]^2)/(x$sdev[comps[1]]^2)+(x$scores[,comps[2]]^2)/(x$sdev[comps[2]]^2))>5.99
points(x$scores[outliers,comps[1]],x$scores[outliers,comps[2]],pch=19,cex=1.2,col="Blue")
text(x$scores[outliers,comps[1]],x$scores[outliers,comps[2]],col="Blue",lab=namevec[outliers], pos=4)
score_outliers <<- as.vector(which(outliers == TRUE))
}
ciscoreplot(pc1,c(1,2), names(working_data))
# outliers <- score_outliers
#
ciscoreplot(pc1,c(1,3), names(working_data))
# outliers <- c(outliers, score_outliers)
#
ciscoreplot(pc1,c(1,4), names(working_data))
# outliers <- c(outliers, score_outliers)
#
ciscoreplot(pc1,c(2,3), names(working_data))
# outliers <- c(outliers, score_outliers)
#
ciscoreplot(pc1,c(2,4), names(working_data))
# outliers <- c(outliers, score_outliers)
#
ciscoreplot(pc1,c(3,4), names(working_data))
# outliers <- c(outliers, score_outliers)
#
# outliers <- unique(outliers)
biplot(pc2,choices=c(1,2),pc.biplot=T)
biplot(pc2,choices=c(1,3),pc.biplot=T)
biplot(pc2,choices=c(1,4),pc.biplot=T)
biplot(pc2,choices=c(2,3),pc.biplot=T)
biplot(pc2,choices=c(2,4),pc.biplot=T)
biplot(pc2,choices=c(3,4),pc.biplot=T)
# working_data <- working_data[-outliers,]
# data_transformed <- data_transformed[-outliers,]
#
# pc3 <- prcomp(data_transformed, scale=TRUE)
# print(summary(pc3),digits=2)
PCA_Data <- pc2$x[,c(1:4)]
Sectors <- droplevels(working_data$sector)
Returns <- working_data$returns
PCA_Data <- data.frame(Returns, Sectors, PCA_Data)
PCA_Data
par(mar=c(3,15,2,2))
boxplot(PC1 ~ Sectors, data=PCA_Data, horizontal = T, main="PC1 by Industry", las=2)
boxplot(PC2 ~ Sectors, data=PCA_Data, horizontal = T, main="PC2 by Industry", las=2)
boxplot(PC3 ~ Sectors, data=PCA_Data, horizontal = T, main="PC3 by Industry", las=2)
boxplot(PC4 ~ Sectors, data=PCA_Data, horizontal = T, main="PC4 by Industry", las=2)
par(mar=c(3,5,2,2))
boxplot(PC1 ~ Returns, data=PCA_Data, horizontal = T, main="PC1 by Returns", las=2)
boxplot(PC2 ~ Returns, data=PCA_Data, horizontal = T, main="PC2 by Returns", las=2)
boxplot(PC3 ~ Returns, data=PCA_Data, horizontal = T, main="PC3 by Returns", las=2)
boxplot(PC4 ~ Returns, data=PCA_Data, horizontal = T, main="PC4 by Returns", las=2)
print("Covariance Matrix for Mediocre")
cov(PCA_Data[PCA_Data$Returns == "Mediocre", 3:6])
cat(c("log-determinant", log(det(cov(PCA_Data[PCA_Data$Returns == "Mediocre", 3:6]))), "\n\n"))
print("Covariance Matrix for High")
cov(PCA_Data[PCA_Data$Returns == "High", 3:6])
cat(c("log-determinant", log(det(cov(PCA_Data[PCA_Data$Returns == "High", 3:6]))), "\n\n"))
print("Covariance Matrix for Low")
cov(PCA_Data[PCA_Data$Returns == "Low", 3:6])
cat(c("log-determinant", log(det(cov(PCA_Data[PCA_Data$Returns == "Low", 3:6]))), "\n\n"))
boxM(PCA_Data[,c("PC1","PC2","PC3","PC4")], PCA_Data$Returns)
CSQPlot(PCA_Data[which(working_data$returns == "Mediocre"),][-c(1,2)], label="Mediocre")
CSQPlot(PCA_Data[which(working_data$returns == "High"),][-c(1,2)], label="High")
CSQPlot(PCA_Data[which(working_data$returns == "Low"),][-c(1,2)], label="Low")
correlations <- c(); j <- 1
for (i in names(data_transformed)){
correlations[i] <- cor(working_data$percent_change, data_transformed[[i]])
}
names(data_transformed)[which.max(abs(correlations))] # roic
data.QDA <- data_transformed # the data
group.QDA <- as.factor(working_data$returns) # the classes
# starting with roic because it has the highest pearson correlation with percent change
stepwise.qda <- stepclass(data.QDA, group.QDA, "qda", start.vars = "roic")
stepwise.qda
plot(stepwise.qda)
# LMAO THIS WAS GARBAGE
manova <- manova(as.matrix(data.QDA) ~ group.QDA)
summary.manova(manova,test="Wilks")
QDA <- qda(data.QDA, group.QDA, CV=FALSE)
table(group.QDA, predict(QDA)$class)
# 72.5% correct
cv.QDA <- qda(data.QDA, group.QDA, CV=TRUE)
table(group.QDA, cv.QDA$class)
# ~62.2% correct
results <- matrix(nrow = 45, ncol = 20)
for (j in 1:20) {
for (i in 1:45) {
test_point <- data.QDA[i,]
train_data <- data.QDA[-i,]
knn_prediction <- as.vector(knn(train = train_data, test = test_point, cl = group.QDA[-i], k = j))
truth <- as.vector(group.QDA[i])
results[i,j] <- truth == knn_prediction
}
}
best_k <- which.max(colMeans(results))
best_k
success_rate <- colMeans(results)[best_k]
success_rate
svm_data <- data.frame(data_transformed, "returns" = as.factor(working_data$returns))
# percent_correct <- c(); misclass <- matrix(rep(0, 9), nrow = 3, ncol = 3)
# for (i in 1:30){
#   withold <- as.vector(sample.int(nrow(svm_data), 30))
#   train <- svm_data[-withold,]
#   test <- svm_data[withold,]
#
#   tune.out <- tune(svm, returns~. , data=train, kernel = "radial", scale = TRUE, ranges = list(cost = c(0.1,1,10), gamma = c(0.5,1)))
#   bestmod <- tune.out$best.model
#
#   predicted_returns.test <- predict(bestmod, test)
#   misclass.test <- table(predict = predicted_returns.test, truth = test$returns)
#   percent_correct[i] <- sum(misclass.test[c(1,4,9)])/30
#   misclass <- misclass + as.matrix(misclass.test)
#   print(i)
# }
#
# mean(percent_correct)
# misclass #out of 900
#
# sum(misclass[c(1,5,9)])/900
# tune.out <- tune(svm, returns~. , data=svm_data, kernel = "radial", cross = 30, scale = TRUE, ranges = list(cost = c(0.1,1,10), gamma = c(0.5,1,2)))
# bestmod <- tune.out$best.model
#
# bestmod$accuracies
# cat("\n")
# bestmod$tot.accuracy
(32+78+73)/332
(28+70+55)/332
partimat(group.QDA~. , data=data.QDA, method="qda")
