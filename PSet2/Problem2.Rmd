---
title: "Problem Set 2"
author: "Liana Wang, Yavuz Ramiz Çolak, Ryo Tamaki, David Lieberman"
date: "2/7/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(ggplot2)
library(tidyr)
library(corrplot)
library(mvtnorm) # References rmvnorm()
library(ellipse) # References ellipse()
library(ggfortify)
```

## PROBLEM 1
1). First, discuss whether your data seems to have a multivariate normal distribution. Make univariate plots (boxplots, normal quantile plots as appropriate). Then make transformations as appropriate. You do NOT need to turn all this in, but describe what you did. THEN make a chi-square quantile plot of the data. Turn in your chi-square quantile plot as appropriate and comment on what you see. NOTE that multivariate normality is NOT a requirement for PCA to work!

```{r}
data <- read.csv("https://pastebin.com/raw/z6pgskch")
data <- data %>% drop_na()

boxplot(data)

data_transformed$regret <- log(data$regret)
data_transformed$notuse <- log(data$notuse)
data_transformed$psycho <- sign(data$psycho)*sqrt(abs(data$psycho))
data_transformed$stoned <- log(data$stoned)
data_transformed$noaspirin <- sqrt(data$noaspirin)
data_transformed$relationship <- sqrt(data$relationship)
data_transformed$lessalcohol <- log(data$lessalcohol)

boxplot(data_transformed)

QQPlot <- function(x, na.rm = TRUE){
  for (i in names(x)) {
    plots <- ggplot(x, aes_string(sample = i)) +
      stat_qq() + stat_qq_line()
    ggsave(plots, filename = paste(i, ".png", sep=""))
  }
}
QQPlot(data_transformed)


```

*We first created a boxplot of our data. There is a pretty significant skew on a few of the variables, which we tried to correct through log and square root transformations. After the transformation, we re-created the boxplots. We observed that all variables were distributed more symmetrically and were closer to normal. Furthermore, after transformation, our resulting QQ-plots appeared to fall mostly along the line that would indicate a roughly normal relationship. It is important to note that while some of our QQ plots were partially piece-wise, that was mainly because we included discrete variables. Discrete variables are relevant for our analysis, so we still wanted to include them. Also, our data set is relatively small. With more data points, the QQplots would more closely follow the 45 degree line.*

```{r}
CSQPlot<-function(vars,label="Chi-Square Quantile Plot"){
   #usually, vars is xxx$residuals or data from one group and label is for plot
     x<-cov(scale(vars),use="pairwise.complete.obs")
     squares<-sort(diag(as.matrix(scale(vars))%*%solve(x)%*%as.matrix(t(scale(vars)))))
     quantiles<-quantile(squares)
     hspr<-quantiles[4]-quantiles[2]
     cumprob<-c(1:length(vars[,1]))/length(vars[,1])-1/(2*length(vars[,1]))
     degf<-dim(x)[1]
     quants<-qchisq(cumprob,df=degf)
     gval<-(quants**(-1+degf/2))/(exp(quants/2)*gamma(degf/2)*(sqrt(2)**degf))
     scale<-hspr / (qchisq(.75,degf)-qchisq(.25,degf))
     se<-(scale/gval)*sqrt(cumprob*(1-cumprob)/length(squares))
     lower<-quants-2*se
     upper<-quants+2*se
    plot(quants,squares,col='red',pch=19,cex=1.2,xlab="Chi-Square Quantiles",
     ylab=label,main=paste("Chi-Square Quantiles for",label),ylim=range(upper,lower, squares) , xlim=range(c(0,quants)))
    lines(c(0,100),c(0,100),col=1)
    lines(quants,upper,col="blue",lty=2,lwd=2)
    lines(quants,lower,col="blue",lty=2,lwd=2)
    legend(0,range(upper,lower)[2]*.9,c("Data","95% Conf Limits"),lty=c(0,2),col=c("red","blue"),lwd=c(2,2),
      pch=c(19,NA))
}
CSQPlot(data_transformed,label="Drug Attitudes")
```

*Our CSQ plot seems to indicate a roughly normal multivariate distribution, with all data (except for 1 variable, it appears) falling within the 95% confidence interval and a good number of variables falling along the line which indicates normality. The relative skew is, as noted above, likely partially due to the small sample size.*

## PROBLEM 2
2). Compute the correlation matrix between all variables (SAS and SPSS will provide this for you as part of the PCA procedure – in SPSS, click on DESCRIPTIVES. In R use the cor() function or one of the other cool correlation plots.). Comment on relationships you do/do not observe. Do you think PCA will work well?
```{r}
round(cor(data_transformed), 2)
#version 1
corrplot(cor(data_transformed), method = "color")
#version 2
corrplot.mixed(cor(data_transformed), lower.col="black", upper = "ellipse", tl.col = "black", number.cex=.7, order = "hclust",tl.pos = "lt", tl.cex=.7)
```
*The correlation plot shows that while there is low correlation between many of  the variables, there are a few spots where variables are moderately or highly correlated. This includes, for example, "notuse" and "relationship"; "trip" and "legal"; "dope" and "notuse," as well as "fun" and "drugscene." Given the noted highly correlated variables, PCA will be helpful in reducing dimensions. It won't necessarily give us two or three highly explanatory variables, but it will reduce the repetitiveness of current variables and allow us to identify trends between them or group them intelligently.* 


## PROBLEM 3
3). Perform Principle components analysis using the Correlation matrix (standardized variables). Think about how many principle components to retain. To make this decision look at
 Total variance explained by a given number of principle components
 The ‘eigenvalue > 1’ criteria
 The ‘scree plot elbow’ method (turn in the scree plot)
 Parallel Analysis : think about whether this is appropriate based on what you
discover in question 1.

```{r}
source("http://www.reuningscherer.net/STAT660/R/parallel.r.txt")
source("http://reuningscherer.net/stat660/r/ciscoreplot.R.txt")
data2 <- data_transformed[,c(names(data_transformed))]
pc1 <- princomp(data2, cor=TRUE)
print(summary(pc1),digits=2,loadings=pc1$loadings,cutoff=0)
round(pc1$sdev^2,2)
```
We look at the cumulative variability accounted by our components:
```{r}
summary(pc1)
```
We also see the screeplot and check for an "elbow" formed by our components:
```{r}
screeplot(pc1,type="lines",col="red",lwd=2,pch=19,cex=1.2,main="Scree Plot of Raw Drug Data")
```

*Recall that Parallel Analysis is appropriate if the data has a multivariate normal distribution. Based on our previous results from (1), in particular the Chi-Squared Quantile plot, we note that our data appears to have a multivariate normal distribution –– all data points (except for 2) are between the 95% confidence interval bounds with respect to the 45-degree line. Consequently, we would say that our data is appropriate for Parallel Analysis, given their multivariate normality.*

```{r}
source("http://www.reuningscherer.net/STAT660/R/parallel.r.txt")
parallelplot(pc1)
```
*Following our analysis, we decided to retain 3 principal components. When we look at the summary of our PCA, we see that by including components 1,2 and 3, we are able to explain 50% of the variability, which is strong given that we are only including three components. Furthermore, following the eigenvalue rule of thumb, we see that the first three components have eigenvalues that are greater than 1 (in fact, much above 2), and the eigenvalues components beyond 3rd rapidly fall near and below 1. The scree plot shows potential "elbows" at the 2nd and 3th component. As such, including the 3rd component in our judgment seems like the best way of maximizing explanatory power while minimizing the number of components to include.*


## PROBLEM 4
4). For principle components you decide to retain, examine the loadings (principle components) and think about an interpretation for each retained component if possible.
```{r}
unclass(pc1$loadings)[,1:6]
```
*Note that following our interpretation of the Scree Plot with Parallel Analysis Thresholds, we decided to retain the first 3 principal components. Looking at the loadings, we note that the 'dope' is the primary contributor to the first principal component (Comp.1), followed closely by 'notuse.' This closeness between 'dope,' whicch relates to  "taking any kind of dope is a pretty dumb idea," and "even if my best friend gave me some hash, I probably wouldn't use it" because a given individual's response to those questions would likely be similar.*

*The second principal component's (Comp.2) primary contributors are 'psycho,' 'noaspirin,' 'dangerous,' 'stoned,' and 'high.' All of the responses to these questions pertain to the safety and social admiration aspect of drug use, which explains their relative closeness in terms of weight.*

*The third principal component's (Comp.3) primary contributors are 'trip,' 'lessalcohol,' 'experience,' 'calm,' 'stupid.' The responses to these questions pertain to the prospect of positive management of drug use.*

## PROBLEM 5
5) Make a score plot of the scores for at least one pair of component scores (one and two, one and three, two and three, etc). Discuss any trends/groupings you observe (probably, this will be ‘none’). As a bonus, try to make a 95% Confidence Ellipse for two of your components. You might want to also try making a bi-plot if you’re using R.

Define the Score Plot Function
```{r}
scoreplot<-function(x,comps,namevec){
  y1<-sqrt(5.99*(x$sdev[comps[1]]^2))
  ymod<-y1-y1%%.05
  y1vec<-c(-y1,seq(-ymod,ymod,by=0.05),y1)
  y2vecpos<-sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
  y2vecneg<--sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
  y2vecpos[1]<-0
  y2vecneg[1]<-0
  y2vecpos[length(y2vecpos)]<-0
  y2vecneg[length(y2vecneg)]<-0
  plot(x$scores[,comps[1]],x$scores[,comps[2]],pch=19,cex=1.2,ylim=c(min(y2vecneg,x$scores[,comps[2]]),max(y2vecpos,x$scores[,comps[2]])),
    main="PC Score Plot", xlab=paste("Scores for PC",comps[1],sep=" "), ylab=paste("Scores for PC",comps[2],sep=" "),
    xlim=c(min(y1vec,x$scores[,comps[1]]),max(y1vec,x$scores[,comps[1]])))
    lines(y1vec,y2vecpos,col="Red",lwd=2)
    lines(y1vec,y2vecneg,col="reducing",lwd=2)
  outliers<-((x$scores[,comps[1]]^2)/(x$sdev[comps[1]]^2)+(x$scores[,comps[2]]^2)/(x$sdev[comps[2]]^2))>5.99
  points(x$scores[outliers,comps[1]],x$scores[outliers,comps[2]],pch=19,cex=1.2,col="Blue")
  text(x$scores[outliers,comps[1]],x$scores[outliers,comps[2]],col="Blue",lab=namevec[outliers], pos=4)
}
```
Let's make the scoreplot for the first two components, including a 95% confidence ellipse in the process:
```{r}
scoreplot(pc1,c(1,2), names(data))
```
*We observe 2 outliers: the variables calm and experience.* 



```{r}
biplot(pc1,choices=c(1,2),pc.biplot=T)
```

## PROBLEM 6
6). Write a paragraph summarizing your findings, and your opinions about the effectiveness of using principle components on this data. Include evidence based on scatterplots of linearity in higher dimensional space, note any multivariate outliers in your score plot, comment on sample size relative to number of variables, etc.
```{r}

```